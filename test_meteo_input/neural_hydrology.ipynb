{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce RTX 3080 Ti\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import xarray as xr\n",
    "from neuralhydrology.nh_run import start_run, eval_run\n",
    "from scripts.file_manipulator import file_rewriter\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "ts_dir = Path('../geo_data/time_series')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run for different configs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERA5 precipitation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-06 08:23:15,375: Logging to runs_no_static/era5_prcp_0604_082315/output.log initialized.\n",
      "2023-04-06 08:23:15,375: ### Folder structure created at runs_no_static/era5_prcp_0604_082315\n",
      "2023-04-06 08:23:15,376: ### Run configurations for era5_prcp\n",
      "2023-04-06 08:23:15,376: experiment_name: era5_prcp\n",
      "2023-04-06 08:23:15,377: run_dir: runs_no_static/era5_prcp_0604_082315\n",
      "2023-04-06 08:23:15,377: train_basin_file: openf_basins.txt\n",
      "2023-04-06 08:23:15,377: validation_basin_file: openf_basins.txt\n",
      "2023-04-06 08:23:15,378: test_basin_file: openf_basins.txt\n",
      "2023-04-06 08:23:15,378: train_start_date: 2009-01-01 00:00:00\n",
      "2023-04-06 08:23:15,379: train_end_date: 2016-12-31 00:00:00\n",
      "2023-04-06 08:23:15,379: validation_start_date: 2017-01-01 00:00:00\n",
      "2023-04-06 08:23:15,380: validation_end_date: 2018-12-31 00:00:00\n",
      "2023-04-06 08:23:15,380: test_start_date: 2019-01-01 00:00:00\n",
      "2023-04-06 08:23:15,380: test_end_date: 2020-12-31 00:00:00\n",
      "2023-04-06 08:23:15,381: per_basin_train_periods_file: None\n",
      "2023-04-06 08:23:15,381: per_basin_validation_periods_file: None\n",
      "2023-04-06 08:23:15,382: per_basin_test_periods_file: None\n",
      "2023-04-06 08:23:15,382: seed: 42\n",
      "2023-04-06 08:23:15,382: device: cuda:0\n",
      "2023-04-06 08:23:15,383: validate_every: 3\n",
      "2023-04-06 08:23:15,383: validate_n_random_basins: 1106\n",
      "2023-04-06 08:23:15,384: cache_validation_data: True\n",
      "2023-04-06 08:23:15,384: metrics: ['NSE', 'KGE']\n",
      "2023-04-06 08:23:15,384: model: cudalstm\n",
      "2023-04-06 08:23:15,385: checkpoint_path: None\n",
      "2023-04-06 08:23:15,385: head: regression\n",
      "2023-04-06 08:23:15,386: output_activation: linear\n",
      "2023-04-06 08:23:15,386: hidden_size: 256\n",
      "2023-04-06 08:23:15,386: initial_forget_bias: 6\n",
      "2023-04-06 08:23:15,387: output_dropout: 0.4\n",
      "2023-04-06 08:23:15,387: optimizer: Adam\n",
      "2023-04-06 08:23:15,387: loss: NSE\n",
      "2023-04-06 08:23:15,388: regularization: None\n",
      "2023-04-06 08:23:15,388: learning_rate: {0: 0.001, 10: 0.0005, 20: 0.0001}\n",
      "2023-04-06 08:23:15,389: batch_size: 256\n",
      "2023-04-06 08:23:15,389: epochs: 30\n",
      "2023-04-06 08:23:15,389: target_noise_std: None\n",
      "2023-04-06 08:23:15,390: clip_gradient_norm: 1\n",
      "2023-04-06 08:23:15,390: predict_last_n: 1\n",
      "2023-04-06 08:23:15,391: seq_length: 365\n",
      "2023-04-06 08:23:15,391: num_workers: 8\n",
      "2023-04-06 08:23:15,391: log_interval: 3\n",
      "2023-04-06 08:23:15,392: log_tensorboard: True\n",
      "2023-04-06 08:23:15,392: log_n_figures: None\n",
      "2023-04-06 08:23:15,393: save_weights_every: 3\n",
      "2023-04-06 08:23:15,393: save_validation_results: True\n",
      "2023-04-06 08:23:15,393: dataset: generic\n",
      "2023-04-06 08:23:15,394: data_dir: ../geo_data\n",
      "2023-04-06 08:23:15,394: save_train_data: True\n",
      "2023-04-06 08:23:15,394: train_data_file: None\n",
      "2023-04-06 08:23:15,395: forcings: None\n",
      "2023-04-06 08:23:15,395: dynamic_inputs: ['t_max_e5', 't_min_e5', 'prcp_e5']\n",
      "2023-04-06 08:23:15,396: target_variables: ['q_mm_day']\n",
      "2023-04-06 08:23:15,396: static_attributes: None\n",
      "2023-04-06 08:23:15,396: additional_feature_files: None\n",
      "2023-04-06 08:23:15,397: evolving_attributes: None\n",
      "2023-04-06 08:23:15,397: use_basin_id_encoding: False\n",
      "2023-04-06 08:23:15,397: number_of_basins: 1106\n",
      "2023-04-06 08:23:15,398: train_dir: runs_no_static/era5_prcp_0604_082315/train_data\n",
      "2023-04-06 08:23:15,398: img_log_dir: runs_no_static/era5_prcp_0604_082315/img_log\n",
      "2023-04-06 08:23:15,399: ### Device cuda:0 will be used for training\n",
      "2023-04-06 08:23:15,400: Loading basin data into xarray data set.\n",
      "100%|██████████| 1106/1106 [00:22<00:00, 49.40it/s]\n",
      "2023-04-06 08:23:38,858: Calculating target variable stds per basin\n",
      " 29%|██▉       | 326/1106 [00:00<00:00, 1657.18it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/neuralhydrology/datasetzoo/basedataset.py:460: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  per_basin_target_stds = torch.tensor([np.nanstd(obs, axis=1)], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1106/1106 [00:00<00:00, 1710.54it/s]\n",
      "2023-04-06 08:23:39,581: Create lookup table and convert to pytorch tensor\n",
      "100%|██████████| 1106/1106 [00:14<00:00, 77.26it/s] \n"
     ]
    }
   ],
   "source": [
    "file_rewriter(q_pathes=glob.glob('../geo_data/great_db/nc_all_q/*.nc'),\n",
    "              ts_dir=ts_dir,\n",
    "              hydro_target='q_mm_day',\n",
    "              meteo_predictors=['t_max_e5', 't_min_e5', 'prcp_e5'])\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"./no_static_configs/era5_qmm.yml\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = Path(\"./runs_q_mm/era5_prcp_0404_074943/\")\n",
    "eval_run(run_dir=run_dir, period=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ERA5-Land precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rewriter(q_pathes=glob.glob('../geo_data/great_db/nc_all_q/*.nc'),\n",
    "              ts_dir=ts_dir,\n",
    "              hydro_target='q_mm_day',\n",
    "              meteo_predictors=['t_max_e5l', 't_min_e5l', 'prcp_e5l'])\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"./no_static_configs/era5Land_qmm.yml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = Path(\"./runs_q_cms/era5_land_prcp_2803_125025\")\n",
    "eval_run(run_dir=run_dir, period=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPCP precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpcp\n",
    "file_rewriter(q_pathes=glob.glob('../geo_data/great_db/nc_all_q/*.nc'),\n",
    "              ts_dir=ts_dir,\n",
    "              hydro_target='q_mm_day',\n",
    "              meteo_predictors=['t_max_e5', 't_min_e5', 'prcp_gpcp'])\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"./no_static_configs/gpcp_qmm.yml\"))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 1136/1136 [02:14<00:00,  8.43it/s]\n"
     ]
    }
   ],
   "source": [
    "run_dir = Path(\"./runs_q_cms/gpcp_prcp_3003_063422\")\n",
    "eval_run(run_dir=run_dir, period=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMERG precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/my_dissertation/test_meteo_input/neural_hydrology.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/my_dissertation/test_meteo_input/neural_hydrology.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m file_rewriter(q_pathes\u001b[39m=\u001b[39;49mglob\u001b[39m.\u001b[39;49mglob(\u001b[39m'\u001b[39;49m\u001b[39m../geo_data/great_db/nc_all_q/*.nc\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/my_dissertation/test_meteo_input/neural_hydrology.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m               ts_dir\u001b[39m=\u001b[39;49mts_dir,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/my_dissertation/test_meteo_input/neural_hydrology.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m               hydro_target\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mq_mm_day\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/my_dissertation/test_meteo_input/neural_hydrology.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m               meteo_predictors\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mt_max_e5\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mt_min_e5\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mprcp_imerg\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/my_dissertation/test_meteo_input/neural_hydrology.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e222c22636f6e66696746696c65223a7b22246d6964223a312c2270617468223a222f686f6d652f64696d612f436f64696e672f6873692f6d795f646973736572746174696f6e2f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a227673636f64652d66696c65486f7374227d7d/workspaces/my_dissertation/test_meteo_input/neural_hydrology.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     start_run(config_file\u001b[39m=\u001b[39mPath(\u001b[39m\"\u001b[39m\u001b[39m./no_static_configs/imerg_qmm.yml\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m/workspaces/my_dissertation/test_meteo_input/scripts/file_manipulator.py:48\u001b[0m, in \u001b[0;36mfile_rewriter\u001b[0;34m(q_pathes, ts_dir, hydro_target, meteo_predictors, possible_nans)\u001b[0m\n\u001b[1;32m     46\u001b[0m     ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mgauge_id\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m     ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39msel()\n\u001b[0;32m---> 48\u001b[0m     ds\u001b[39m.\u001b[39;49mto_netcdf(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mts_dir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mfilename\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     49\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/core/dataset.py:1900\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     encoding \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1898\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 1900\u001b[0m \u001b[39mreturn\u001b[39;00m to_netcdf(\n\u001b[1;32m   1901\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1902\u001b[0m     path,\n\u001b[1;32m   1903\u001b[0m     mode,\n\u001b[1;32m   1904\u001b[0m     \u001b[39mformat\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mformat\u001b[39;49m,\n\u001b[1;32m   1905\u001b[0m     group\u001b[39m=\u001b[39;49mgroup,\n\u001b[1;32m   1906\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[1;32m   1907\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1908\u001b[0m     unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims,\n\u001b[1;32m   1909\u001b[0m     compute\u001b[39m=\u001b[39;49mcompute,\n\u001b[1;32m   1910\u001b[0m     invalid_netcdf\u001b[39m=\u001b[39;49minvalid_netcdf,\n\u001b[1;32m   1911\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/api.py:1072\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39m# TODO: figure out how to refactor this logic (here and in save_mfdataset)\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39m# to avoid this mess of conditionals\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1070\u001b[0m     \u001b[39m# TODO: allow this work (setting up the file for writing array data)\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m     \u001b[39m# to be parallelized with dask\u001b[39;00m\n\u001b[0;32m-> 1072\u001b[0m     dump_to_store(\n\u001b[1;32m   1073\u001b[0m         dataset, store, writer, encoding\u001b[39m=\u001b[39;49mencoding, unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[1;32m   1075\u001b[0m     \u001b[39mif\u001b[39;00m autoclose:\n\u001b[1;32m   1076\u001b[0m         store\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/api.py:1119\u001b[0m, in \u001b[0;36mdump_to_store\u001b[0;34m(dataset, store, writer, encoder, encoding, unlimited_dims)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[39mif\u001b[39;00m encoder:\n\u001b[1;32m   1117\u001b[0m     variables, attrs \u001b[39m=\u001b[39m encoder(variables, attrs)\n\u001b[0;32m-> 1119\u001b[0m store\u001b[39m.\u001b[39;49mstore(variables, attrs, check_encoding, writer, unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/common.py:265\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.store\u001b[0;34m(self, variables, attributes, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_dimensions(variables, unlimited_dims\u001b[39m=\u001b[39munlimited_dims)\n\u001b[0;32m--> 265\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_variables(\n\u001b[1;32m    266\u001b[0m     variables, check_encoding_set, writer, unlimited_dims\u001b[39m=\u001b[39;49munlimited_dims\n\u001b[1;32m    267\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/common.py:307\u001b[0m, in \u001b[0;36mAbstractWritableDataStore.set_variables\u001b[0;34m(self, variables, check_encoding_set, writer, unlimited_dims)\u001b[0m\n\u001b[1;32m    302\u001b[0m check \u001b[39m=\u001b[39m vn \u001b[39min\u001b[39;00m check_encoding_set\n\u001b[1;32m    303\u001b[0m target, source \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_variable(\n\u001b[1;32m    304\u001b[0m     name, v, check, unlimited_dims\u001b[39m=\u001b[39munlimited_dims\n\u001b[1;32m    305\u001b[0m )\n\u001b[0;32m--> 307\u001b[0m writer\u001b[39m.\u001b[39;49madd(source, target)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/common.py:156\u001b[0m, in \u001b[0;36mArrayWriter.add\u001b[0;34m(self, source, target, region)\u001b[0m\n\u001b[1;32m    154\u001b[0m     target[region] \u001b[39m=\u001b[39m source\n\u001b[1;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     target[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m=\u001b[39m source\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/xarray/backends/netCDF4_.py:72\u001b[0m, in \u001b[0;36mBaseNetCDF4Array.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatastore\u001b[39m.\u001b[39mlock:\n\u001b[1;32m     71\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_array(needs_lock\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 72\u001b[0m     data[key] \u001b[39m=\u001b[39m value\n\u001b[1;32m     73\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatastore\u001b[39m.\u001b[39mautoclose:\n\u001b[1;32m     74\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatastore\u001b[39m.\u001b[39mclose(needs_lock\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_rewriter(q_pathes=glob.glob('../geo_data/great_db/nc_all_q/*.nc'),\n",
    "              ts_dir=ts_dir,\n",
    "              hydro_target='q_mm_day',\n",
    "              meteo_predictors=['t_max_e5', 't_min_e5', 'prcp_imerg'])\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"./no_static_configs/imerg_qmm.yml\"))\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = Path(\"./runs_q_cms/imerg_prcp_3003_135405\")\n",
    "eval_run(run_dir=run_dir, period=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSWEP precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rewriter(q_pathes=glob.glob('../geo_data/great_db/nc_all_q/*.nc'),\n",
    "              ts_dir=ts_dir,\n",
    "              hydro_target='q_mm_day',\n",
    "              meteo_predictors=['t_max_e5l', 't_min_e5l', 'prcp_mswep'])\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"./no_static_configs/mswep_qmm.yml\"))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 1136/1136 [02:14<00:00,  8.46it/s]\n"
     ]
    }
   ],
   "source": [
    "run_dir = Path(\"./runs_q_cms/mswep_prcp_3103_073833\")\n",
    "eval_run(run_dir=run_dir, period=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rewriter(q_pathes=glob.glob('../geo_data/great_db/nc_all_h/*.nc'),\n",
    "              ts_dir=ts_dir,\n",
    "              hydro_target='lvl_sm',\n",
    "              meteo_predictors=['t_max_e5l', 't_min_e5l', 'prcp_e5l'])\n",
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"./configs/e5l_sm.yml\"))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blind forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-05 06:22:00,239: PROJ: internal_proj_identify: /opt/conda/share/proj/proj.db contains DATABASE.LAYOUT.VERSION.MINOR = 0 whereas a number >= 2 is expected. It comes from another PROJ installation.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "partial_gauges = gpd.read_file(\n",
    "    '../geo_data/great_db/geometry/gauges_partial_q.gpkg')\n",
    "\n",
    "with open('./basins_test.txt', 'w') as the_file:\n",
    "    for gauge_name in partial_gauges['gauge_id']:\n",
    "        the_file.write(f'{int(gauge_name)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_pathes = glob.glob('../geo_data/great_db/nc_concat/*.nc')\n",
    "for file in q_pathes:\n",
    "    gauge_id = file.split('/')[-1][:-3]\n",
    "    if gauge_id in list(partial_gauges['gauge_id']):\n",
    "        ds = xr.open_dataset(file)\n",
    "        filename = file.split('/')[-1]\n",
    "        try:\n",
    "            ds = ds.drop('gauge_id')\n",
    "            ds.to_netcdf(f'{ts_dir}/{filename}')\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "file_rewriter(q_pathes=glob.glob('../geo_data/great_db/nc_all_q/*.nc'),\n",
    "              ts_dir=ts_dir,\n",
    "              hydro_target='q_mm_day',\n",
    "              meteo_predictors=['t_max_e5l', 't_min_e5l', 'prcp_e5l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    start_run(config_file=Path(\"./configs/e5l_mbs.yml\"))\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
