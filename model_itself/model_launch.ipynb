{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_builder import HydroForecastData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll utlize cpu in further calculations\n"
     ]
    }
   ],
   "source": [
    "geo_folder = '../geo_data/great_db'\n",
    "\n",
    "forecast_data = HydroForecastData(\n",
    "    nc_files=glob.glob(f'{geo_folder}/nc_concat/*.nc')[:5],\n",
    "    gauges=['5746', '3159', '8376'],\n",
    "    predictors=['t_max_e5l', 't_min_e5l', 'prcp_e5l'],\n",
    "    target=['lvl_sm'],\n",
    "    hydroatlas_path='../geo_data/static_attributes/geo_vector.csv',\n",
    "    hydroatlas=['for_pc_sse', 'crp_pc_sse', 'inu_pc_ult',\n",
    "                'ire_pc_sse', 'lka_pc_use', 'prm_pc_sse',\n",
    "                'pst_pc_sse', 'cly_pc_sav', 'slt_pc_sav',\n",
    "                'snd_pc_sav', 'kar_pc_sse', 'urb_pc_sse',\n",
    "                'gwt_cm_sav', 'lkv_mc_usu', 'rev_mc_usu',\n",
    "                'sgr_dk_sav', 'slp_dg_sav', 'ws_area',\n",
    "                'ele_mt_sav'],\n",
    "    h_bs_file='../ais_parsers/data/height_gauge.csv',\n",
    "    future_interval=7, past_interval=365,\n",
    "    train_start='01/01/2008', train_end='12/31/2015',\n",
    "    val_start='01/01/2016', val_end='12/31/2018',\n",
    "    test_start='01/01/2019', test_end='12/31/2020')\n",
    "# store static data for further concat in decoder\n",
    "static_attributes = forecast_data.static_attributes\n",
    "# Here we are defining properties for our model\n",
    "\n",
    "# Training batch size\n",
    "BATCH_SIZE = 128\n",
    "# so called torch Datasets\n",
    "train_ds, val_ds, test_ds = forecast_data.train_val_test()\n",
    "# Split the data according to our split ratio and load each subset into a\n",
    "# separate DataLoader object\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, drop_last=True)\n",
    "\n",
    "dataloaders = {x: dl for x, dl\n",
    "               in zip(['train', 'val', 'test'],\n",
    "                      [train_loader, val_loader, test_loader])}\n",
    "datasizes = {x: len(dl) for x, dl in dataloaders.items()}\n",
    "# Observe that all parameters are being optimized\n",
    "# Device selection (CPU | GPU)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = 'cuda' if USE_CUDA else 'cpu'\n",
    "print(f\"We'll utlize {device} in further calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
