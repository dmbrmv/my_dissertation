{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465c6ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 996 gauges and 996 watersheds\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from scipy.ndimage import median_filter\n",
    "from scipy.signal import savgol_filter\n",
    "import xarray as xr\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.analytics.cluster_hex_analysis import (\n",
    "    aggregate_clusters_to_hex,\n",
    "    assign_gauges_to_hybrid_classes,\n",
    "    consolidate_hybrid_combinations,\n",
    "    map_secondary_cluster_to_hex,\n",
    ")\n",
    "from src.plots.hex_utils import build_hex_grid, to_equal_area\n",
    "from src.readers.geom_reader import load_geodata\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DeJavu Serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
    "\n",
    "log = setup_logger(\"hybrid_analysis\", log_file=\"../logs/hybrid_analysis.log\")\n",
    "\n",
    "# Load geometry\n",
    "ws, gauges = load_geodata(folder_depth=\"../\")\n",
    "gauges.index = gauges.index.map(str)\n",
    "ws.index = ws.index.map(str)\n",
    "\n",
    "print(f\"Loaded {len(gauges)} gauges and {len(ws)} watersheds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517efd11",
   "metadata": {},
   "source": [
    "## 1. Load Geographical Clusters (9 clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928337f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geographic clustering: 9 clusters for 996 gauges\n"
     ]
    }
   ],
   "source": [
    "# Load HydroATLAS data\n",
    "geo_data = pd.read_csv(\n",
    "    \"../data/attributes/hydro_atlas_cis_camels.csv\",\n",
    "    dtype={\"gauge_id\": str},\n",
    ")\n",
    "\n",
    "if \"gauge_id\" in geo_data.columns:\n",
    "    geo_data = geo_data.set_index(\"gauge_id\")\n",
    "\n",
    "geo_data.index = geo_data.index.map(str)\n",
    "\n",
    "# Static parameters for clustering\n",
    "static_parameters = [\n",
    "    \"for_pc_use\",\n",
    "    \"crp_pc_use\",\n",
    "    \"inu_pc_ult\",\n",
    "    \"ire_pc_use\",\n",
    "    \"lka_pc_use\",\n",
    "    \"prm_pc_use\",\n",
    "    \"pst_pc_use\",\n",
    "    \"cly_pc_uav\",\n",
    "    \"slt_pc_uav\",\n",
    "    \"snd_pc_uav\",\n",
    "    \"kar_pc_use\",\n",
    "    \"urb_pc_use\",\n",
    "    \"gwt_cm_sav\",\n",
    "    \"lkv_mc_usu\",\n",
    "    \"rev_mc_usu\",\n",
    "    \"slp_dg_uav\",\n",
    "    \"sgr_dk_sav\",\n",
    "    \"ws_area\",\n",
    "    \"ele_mt_uav\",\n",
    "]\n",
    "\n",
    "# Align indices\n",
    "available = pd.Index(gauges.index).intersection(geo_data.index)\n",
    "gauges = gauges.loc[available].copy()\n",
    "ws = ws.loc[available].copy()\n",
    "common_index = gauges.index.to_list()\n",
    "\n",
    "geo_subset = geo_data.loc[common_index, static_parameters]\n",
    "geo_scaled = (geo_subset - geo_subset.min()) / (geo_subset.max() - geo_subset.min())\n",
    "\n",
    "# Hierarchical clustering\n",
    "n_geo_clusters = 9\n",
    "Z_geo = linkage(geo_scaled.values, method=\"ward\", metric=\"euclidean\")\n",
    "geo_labels = fcluster(Z_geo, t=n_geo_clusters, criterion=\"maxclust\")\n",
    "\n",
    "print(f\"Geographic clustering: {n_geo_clusters} clusters for {len(common_index)} gauges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0759d2bc",
   "metadata": {},
   "source": [
    "## 2. Load Hydrological Clusters (10 clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776331c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hydrological clustering: 10 clusters for 996 gauges\n"
     ]
    }
   ],
   "source": [
    "# Load discharge time series\n",
    "discharge_data = {}\n",
    "dis_column = \"q_mm_day\"\n",
    "\n",
    "for gauge_id in ws.index:\n",
    "    try:\n",
    "        with xr.open_dataset(f\"../data/nc_all_q/{gauge_id}.nc\") as ds:\n",
    "            df = ds.to_dataframe()\n",
    "        discharge_data[gauge_id] = df[[dis_column]].squeeze()\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error loading discharge for {gauge_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate median seasonal cycle\n",
    "q_df = {}\n",
    "for gauge_id in discharge_data.keys():\n",
    "    try:\n",
    "        ts = discharge_data[gauge_id]\n",
    "        seasonal_cycle = ts.groupby([ts.index.month, ts.index.day]).median().values\n",
    "        q_df[gauge_id] = seasonal_cycle\n",
    "    except Exception as e:\n",
    "        log.error(f\"Error calculating seasonal cycle for {gauge_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "q_df = pd.DataFrame.from_dict(q_df, orient=\"columns\")\n",
    "q_df = q_df.loc[:, q_df.max() < 50]\n",
    "\n",
    "# Spike removal and smoothing\n",
    "q_df_smoothed = q_df.copy()\n",
    "for col in q_df_smoothed.columns:\n",
    "    series = q_df_smoothed[col].values\n",
    "    smoothed = median_filter(series, size=5, mode=\"wrap\")\n",
    "    residuals = np.abs(series - smoothed)\n",
    "    threshold = residuals.std() * 3\n",
    "    spike_mask = residuals > threshold\n",
    "\n",
    "    if spike_mask.any():\n",
    "        valid_indices = np.where(~spike_mask)[0]\n",
    "        spike_indices = np.where(spike_mask)[0]\n",
    "        if len(valid_indices) > 1:\n",
    "            interpolated_values = np.interp(\n",
    "                spike_indices, valid_indices, series[valid_indices]\n",
    "            )\n",
    "            series[spike_indices] = interpolated_values\n",
    "\n",
    "    series_smoothed = savgol_filter(series, window_length=11, polyorder=3, mode=\"wrap\")\n",
    "    q_df_smoothed[col] = series_smoothed\n",
    "\n",
    "q_df = q_df_smoothed.copy()\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "q_df_normalized = (q_df - q_df.min()) / (q_df.max() - q_df.min())\n",
    "\n",
    "# Prepare for clustering (rows=gauges, columns=days)\n",
    "q_df_clust = q_df_normalized.T.copy()\n",
    "\n",
    "# Add spatial coordinates\n",
    "for gauge_id in q_df_clust.index:\n",
    "    q_df_clust.loc[gauge_id, \"lat\"] = gauges.loc[gauge_id, \"geometry\"].y\n",
    "    q_df_clust.loc[gauge_id, \"lon\"] = gauges.loc[gauge_id, \"geometry\"].x\n",
    "\n",
    "q_df_clust = q_df_clust.dropna()\n",
    "hydro_index = q_df_clust.index\n",
    "\n",
    "# Hierarchical clustering\n",
    "n_hydro_clusters = 10\n",
    "Z_hydro = linkage(q_df_clust.values, method=\"ward\", metric=\"euclidean\")\n",
    "hydro_labels = fcluster(Z_hydro, t=n_hydro_clusters, criterion=\"maxclust\")\n",
    "\n",
    "print(\n",
    "    f\"Hydrological clustering: {n_hydro_clusters} clusters for {len(hydro_index)} gauges\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269507d1",
   "metadata": {},
   "source": [
    "## 3. Create Raw Hybrid Combinations (Ф#-Г#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c14af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Raw hybrid combinations: 51\n",
      "Gauges per combination (mean): 19.5\n",
      "Gauges per combination (median): 11\n",
      "\n",
      "Top 10 combinations:\n",
      "hybrid_combo\n",
      "G1-H5    68\n",
      "G5-H3    66\n",
      "G9-H7    64\n",
      "G1-H1    52\n",
      "G1-H3    52\n",
      "G4-H2    50\n",
      "G3-H8    50\n",
      "G9-H9    44\n",
      "G8-H1    43\n",
      "G5-H2    43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create gauge-level dataframe with both clusters\n",
    "gauge_clustered = gauges.copy()\n",
    "gauge_clustered[\"geo_cluster\"] = [f\"G{cl}\" for cl in geo_labels]\n",
    "gauge_clustered[\"hydro_cluster\"] = [f\"H{cl}\" for cl in hydro_labels]\n",
    "gauge_clustered[\"hybrid_combo\"] = (\n",
    "    gauge_clustered[\"geo_cluster\"] + \"-\" + gauge_clustered[\"hydro_cluster\"]\n",
    ")\n",
    "\n",
    "# Apply Russian naming\n",
    "gauge_clustered[\"geo_cluster_ru\"] = gauge_clustered[\"geo_cluster\"].str.replace(\"G\", \"Ф\")\n",
    "gauge_clustered[\"hydro_cluster_ru\"] = gauge_clustered[\"hydro_cluster\"].str.replace(\n",
    "    \"H\", \"Г\"\n",
    ")\n",
    "gauge_clustered[\"hybrid_combo_ru\"] = (\n",
    "    gauge_clustered[\"geo_cluster_ru\"] + \"-\" + gauge_clustered[\"hydro_cluster_ru\"]\n",
    ")\n",
    "\n",
    "combo_counts = gauge_clustered[\"hybrid_combo\"].value_counts()\n",
    "print(f\"\\nRaw hybrid combinations: {len(combo_counts)}\")\n",
    "print(f\"Gauges per combination (mean): {combo_counts.mean():.1f}\")\n",
    "print(f\"Gauges per combination (median): {combo_counts.median():.0f}\")\n",
    "print(\"\\nTop 10 combinations:\")\n",
    "print(combo_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da5844",
   "metadata": {},
   "source": [
    "## 4. Consolidate via Hex-Based Aggregation (~16 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03e33dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1897 hexagons with radius ~80 km\n",
      "Hexes with dominant geo cluster: 231\n",
      "\n",
      "Unique hex-level combinations: 37\n"
     ]
    }
   ],
   "source": [
    "# Transform to equal-area projection\n",
    "aea_crs = (\n",
    "    \"+proj=aea +lat_0=56 +lon_0=100 +lat_1=50 +lat_2=70 \"\n",
    "    \"+x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\"\n",
    ")\n",
    "\n",
    "ws_aea = to_equal_area(ws, crs=aea_crs)\n",
    "gauges_aea = to_equal_area(gauge_clustered, crs=aea_crs)\n",
    "\n",
    "# Build hexagonal grid\n",
    "from shapely.geometry import box\n",
    "\n",
    "extent_poly = box(*ws_aea.total_bounds)\n",
    "hex_grid = build_hex_grid(extent_poly, r_km=80, crs=aea_crs)\n",
    "\n",
    "print(f\"Created {len(hex_grid)} hexagons with radius ~80 km\")\n",
    "\n",
    "# Aggregate geo clusters to hexes\n",
    "ws_with_geo = ws_aea.copy()\n",
    "ws_with_geo[\"geo_cluster\"] = gauge_clustered.loc[ws_aea.index, \"geo_cluster\"]\n",
    "\n",
    "hexes_with_geo = aggregate_clusters_to_hex(\n",
    "    watersheds=ws_with_geo,\n",
    "    hexes=hex_grid,\n",
    "    cluster_col=\"geo_cluster\",\n",
    "    method=\"dominant\",\n",
    "    min_watersheds=2,\n",
    ")\n",
    "\n",
    "print(f\"Hexes with dominant geo cluster: {len(hexes_with_geo)}\")\n",
    "\n",
    "# Map hydro clusters to hexes\n",
    "ws_with_hydro = ws_aea.copy()\n",
    "ws_with_hydro[\"hydro_cluster\"] = gauge_clustered.loc[ws_aea.index, \"hydro_cluster\"]\n",
    "\n",
    "hexes_with_both = map_secondary_cluster_to_hex(\n",
    "    watersheds=ws_with_hydro,\n",
    "    hexes_with_primary=hexes_with_geo,\n",
    "    primary_cluster_col=\"dominant_cluster\",\n",
    "    secondary_cluster_col=\"hydro_cluster\",\n",
    "    hex_id_col=\"hex_id\",\n",
    ")\n",
    "\n",
    "# Create hybrid combo on hex level\n",
    "hexes_with_both[\"hybrid_combo\"] = (\n",
    "    hexes_with_both[\"dominant_cluster\"].astype(str)\n",
    "    + \"-\"\n",
    "    + hexes_with_both[\"dominant_hydro_cluster\"].astype(str)\n",
    ")\n",
    "\n",
    "hex_combo_counts = hexes_with_both[\"hybrid_combo\"].value_counts()\n",
    "print(f\"\\nUnique hex-level combinations: {len(hex_combo_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b75b3",
   "metadata": {},
   "source": [
    "## 5. Assign Gauges to Consolidated Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56534850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauges with hybrid combos: 996\n",
      "Unique combinations: 37\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "Final hybrid classes: 16\n",
      "Gauges per class (mean): 62.2\n",
      "\n",
      "Class distribution:\n",
      "hybrid_class\n",
      "G1-H1        46\n",
      "G1-H3        66\n",
      "G1-H5       110\n",
      "G2-H4        33\n",
      "G3-H8       115\n",
      "G4-H2        54\n",
      "G5-H1        51\n",
      "G5-H10       40\n",
      "G5-H2        41\n",
      "G5-H3       100\n",
      "G6-H2        62\n",
      "G8-H1        75\n",
      "G8-H10       40\n",
      "G9-H7       105\n",
      "G9-H9        43\n",
      "Other-G7     15\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assign gauges to hex-based hybrid classes\n",
    "gauges_with_combos = assign_gauges_to_hybrid_classes(\n",
    "    gauges=gauges_aea,\n",
    "    watersheds=ws_aea,\n",
    "    hexes_with_combos=hexes_with_both,\n",
    "    combo_col=\"hybrid_combo\",\n",
    "    hex_id_col=\"hex_id\",\n",
    "    k_neighbors=5,\n",
    ")\n",
    "\n",
    "print(f\"Gauges with hybrid combos: {gauges_with_combos['hybrid_combo'].notna().sum()}\")\n",
    "print(f\"Unique combinations: {gauges_with_combos['hybrid_combo'].nunique()}\")\n",
    "\n",
    "# Consolidate to target N classes\n",
    "gauges_final, mapping_df = consolidate_hybrid_combinations(\n",
    "    gauges_with_combos=gauges_with_combos,\n",
    "    combo_col=\"hybrid_combo\",\n",
    "    target_n_classes=15,\n",
    "    min_gauges_per_class=5,\n",
    ")\n",
    "\n",
    "print(\"\\n=== FINAL RESULT ===\")\n",
    "print(f\"Final hybrid classes: {gauges_final['hybrid_class'].nunique()}\")\n",
    "print(\n",
    "    f\"Gauges per class (mean): {gauges_final['hybrid_class'].value_counts().mean():.1f}\"\n",
    ")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(gauges_final[\"hybrid_class\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd178c",
   "metadata": {},
   "source": [
    "## 6. Apply Russian Naming to Final Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24287f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final classes with Russian naming:\n",
      "hybrid_class_ru\n",
      "Other-Ф7     15\n",
      "Ф1-Г1        46\n",
      "Ф1-Г3        66\n",
      "Ф1-Г5       110\n",
      "Ф2-Г4        33\n",
      "Ф3-Г8       115\n",
      "Ф4-Г2        54\n",
      "Ф5-Г1        51\n",
      "Ф5-Г10       40\n",
      "Ф5-Г2        41\n",
      "Ф5-Г3       100\n",
      "Ф6-Г2        62\n",
      "Ф8-Г1        75\n",
      "Ф8-Г10       40\n",
      "Ф9-Г7       105\n",
      "Ф9-Г9        43\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert back to WGS84\n",
    "gauges_final_wgs84 = gauges_final.to_crs(gauges.crs)\n",
    "gauges_final_wgs84.index = gauge_clustered.index\n",
    "\n",
    "\n",
    "# Apply Russian naming to hybrid_class\n",
    "def convert_to_russian(hybrid_class: str) -> str:\n",
    "    \"\"\"Convert G#-H# format to Ф#-Г# format.\"\"\"\n",
    "    if pd.isna(hybrid_class):\n",
    "        return hybrid_class\n",
    "    return hybrid_class.replace(\"G\", \"Ф\").replace(\"H\", \"Г\")\n",
    "\n",
    "\n",
    "gauges_final_wgs84[\"hybrid_class_ru\"] = gauges_final_wgs84[\"hybrid_class\"].apply(\n",
    "    convert_to_russian\n",
    ")\n",
    "\n",
    "# Merge back to gauge_clustered\n",
    "gauge_clustered = gauge_clustered.join(\n",
    "    gauges_final_wgs84[[\"hybrid_class\", \"hybrid_class_ru\"]], how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"\\nFinal classes with Russian naming:\")\n",
    "print(gauge_clustered[\"hybrid_class_ru\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e76500",
   "metadata": {},
   "source": [
    "## 7. Extract DOY-Based Peak Timing from Discharge Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e69766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hydrological Regime Characteristics:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['cluster_name', 'peak_doy', 'regime_type', 'cv'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m regime_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(regime_data)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHydrological Regime Characteristics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mregime_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcluster_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpeak_doy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregime_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/geo/lib/python3.10/site-packages/pandas/core/frame.py:4119\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4118\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4119\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4121\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/geo/lib/python3.10/site-packages/pandas/core/indexes/base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/geo/lib/python3.10/site-packages/pandas/core/indexes/base.py:6261\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6263\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['cluster_name', 'peak_doy', 'regime_type', 'cv'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Calculate peak DOY for each hydrological cluster\n",
    "from collections import OrderedDict\n",
    "\n",
    "cluster_q_mm = OrderedDict()\n",
    "for cluster_id in range(1, n_hydro_clusters + 1):\n",
    "    cluster_gauges = gauge_clustered[\n",
    "        gauge_clustered[\"hydro_cluster\"] == f\"G{cluster_id}\"\n",
    "    ].index\n",
    "    if len(cluster_gauges) == 0:\n",
    "        continue\n",
    "\n",
    "    # Get normalized patterns for this cluster\n",
    "    cluster_patterns = q_df_normalized[cluster_gauges]\n",
    "    median_pattern = cluster_patterns.median(axis=1)\n",
    "    cluster_q_mm[f\"Г{cluster_id}\"] = median_pattern.values\n",
    "\n",
    "# Extract regime characteristics\n",
    "regime_data = []\n",
    "for cluster_name, pattern in cluster_q_mm.items():\n",
    "    peak_doy = np.argmax(pattern) + 1  # +1 for 1-based DOY\n",
    "    peak_value = np.max(pattern)\n",
    "\n",
    "    # Seasonal ratios\n",
    "    winter = pattern[:59].mean()  # Jan-Feb\n",
    "    spring = pattern[59:151].mean()  # Mar-May\n",
    "    summer = pattern[151:243].mean()  # Jun-Aug\n",
    "    autumn = pattern[243:334].mean()  # Sep-Nov\n",
    "\n",
    "    # Determine regime type\n",
    "    if peak_doy <= 151:  # Before June\n",
    "        regime_type = \"Spring-dominated (snowmelt)\"\n",
    "    elif peak_doy <= 243:  # June-Aug\n",
    "        regime_type = \"Summer-dominated (rain/glacier)\"\n",
    "    else:\n",
    "        regime_type = \"Autumn-dominated (rain)\"\n",
    "\n",
    "    cv = np.std(pattern) / np.mean(pattern)\n",
    "\n",
    "    regime_data.append(\n",
    "        {\n",
    "            \"cluster_name\": cluster_name,\n",
    "            \"peak_doy\": peak_doy,\n",
    "            \"peak_value\": peak_value,\n",
    "            \"regime_type\": regime_type,\n",
    "            \"winter_ratio\": winter,\n",
    "            \"spring_ratio\": spring,\n",
    "            \"summer_ratio\": summer,\n",
    "            \"autumn_ratio\": autumn,\n",
    "            \"cv\": cv,\n",
    "        }\n",
    "    )\n",
    "\n",
    "regime_df = pd.DataFrame(regime_data)\n",
    "print(\"\\nHydrological Regime Characteristics:\")\n",
    "print(regime_df[[\"cluster_name\", \"peak_doy\", \"regime_type\", \"cv\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91b1e2",
   "metadata": {},
   "source": [
    "## 8. Export Gauge Mapping with Hex Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92760770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final export dataframe\n",
    "export_df = gauge_clustered[\n",
    "    [\"geo_cluster_ru\", \"hydro_cluster_ru\", \"hybrid_combo_ru\", \"hybrid_class_ru\"]\n",
    "].copy()\n",
    "export_df = export_df.reset_index().rename(columns={\"index\": \"gauge_id\"})\n",
    "\n",
    "# Add hex_id from gauges_final_wgs84\n",
    "if \"hex_id\" in gauges_final_wgs84.columns:\n",
    "    export_df = export_df.merge(\n",
    "        gauges_final_wgs84.reset_index()[[\"index\", \"hex_id\"]],\n",
    "        left_on=\"gauge_id\",\n",
    "        right_on=\"index\",\n",
    "        how=\"left\",\n",
    "    ).drop(columns=[\"index\"])\n",
    "\n",
    "# Export to CSV\n",
    "output_dir = Path(\"../res/chapter_one\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_file = output_dir / \"gauge_hybrid_mapping.csv\"\n",
    "export_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✓ Exported gauge mapping to {output_file}\")\n",
    "print(f\"Columns: {list(export_df.columns)}\")\n",
    "print(f\"Total gauges: {len(export_df)}\")\n",
    "print(\"\\nSample rows:\")\n",
    "print(export_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc6d10",
   "metadata": {},
   "source": [
    "## 9. Generate YAML Reference Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cef384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Build YAML structure\n",
    "yaml_data = {\n",
    "    \"hybrid_classification\": {\n",
    "        \"description\": \"Hybrid geo-hydrological classification of Russian watersheds\",\n",
    "        \"n_geo_clusters\": n_geo_clusters,\n",
    "        \"n_hydro_clusters\": n_hydro_clusters,\n",
    "        \"n_final_classes\": int(gauges_final[\"hybrid_class\"].nunique()),\n",
    "        \"naming_convention\": {\n",
    "            \"geo\": \"Ф# (Физико-географический кластер)\",\n",
    "            \"hydro\": \"Г# (Гидрологический кластер)\",\n",
    "            \"hybrid\": \"Ф#-Г# (гибридная комбинация)\",\n",
    "        },\n",
    "        \"classes\": {},\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add class details\n",
    "for hybrid_class in sorted(gauges_final[\"hybrid_class\"].dropna().unique()):\n",
    "    class_gauges = gauge_clustered[gauge_clustered[\"hybrid_class\"] == hybrid_class]\n",
    "    n_gauges = len(class_gauges)\n",
    "\n",
    "    # Get Russian name\n",
    "    class_ru = class_gauges[\"hybrid_class_ru\"].iloc[0] if len(class_gauges) > 0 else \"\"\n",
    "\n",
    "    # Extract geo and hydro parts\n",
    "    parts = hybrid_class.split(\"-\")\n",
    "    geo_part = parts[0] if len(parts) > 0 else \"\"\n",
    "    hydro_part = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "    # Get regime info if available\n",
    "    regime_info = None\n",
    "    hydro_ru = hydro_part.replace(\"H\", \"Г\")\n",
    "    regime_match = regime_df[regime_df[\"cluster_name\"] == hydro_ru]\n",
    "    if not regime_match.empty:\n",
    "        regime_info = {\n",
    "            \"peak_doy\": int(regime_match.iloc[0][\"peak_doy\"]),\n",
    "            \"regime_type\": regime_match.iloc[0][\"regime_type\"],\n",
    "            \"cv\": float(regime_match.iloc[0][\"cv\"]),\n",
    "        }\n",
    "\n",
    "    yaml_data[\"hybrid_classification\"][\"classes\"][class_ru] = {\n",
    "        \"n_gauges\": n_gauges,\n",
    "        \"geo_cluster\": geo_part.replace(\"G\", \"Ф\"),\n",
    "        \"hydro_cluster\": hydro_ru,\n",
    "        \"regime\": regime_info,\n",
    "    }\n",
    "\n",
    "# Export YAML\n",
    "yaml_file = output_dir / \"hybrid_classification_reference.yml\"\n",
    "with open(yaml_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(yaml_data, f, allow_unicode=True, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "print(f\"\\n✓ Exported YAML reference to {yaml_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
