{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace lighting.pytorch as pytorch_lightning\n",
    "from pytorch_forecasting import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_builder import HydroForecastData\n",
    "from scripts.tft_data import file_checker, open_for_tft, train_val_split\n",
    "from scripts.model_eval import pred_res_builder\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import (TimeSeriesDataSet, TemporalFusionTransformer,\n",
    "                                 Baseline)\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss, SMAPE, RMSE, MASE\n",
    "from pytorch_forecasting.metrics.base_metrics import MultiHorizonMetric\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "\n",
    "# torch.set_float32_matmul_precision('medium')\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')\n",
    "meteo_input = ['prcp_e5l',  't_max_e5l', 't_min_e5l']\n",
    "hydro_target = 'q_mm_day'\n",
    "\n",
    "ws_file = gpd.read_file('../geo_data/great_db/geometry/russia_ws.gpkg')\n",
    "ws_file = ws_file.set_index('gauge_id')\n",
    "# ws_file = ws_file[ws_file['new_area'] <= 50000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretation_for_gauge(interp_dict: dict,\n",
    "                             static_parameters: list,\n",
    "                             encoder_params: list,\n",
    "                             decoder_params: list):\n",
    "\n",
    "    def to_percentage(values: torch.Tensor):\n",
    "        values = values / values.sum(-1).unsqueeze(-1)\n",
    "        return values\n",
    "\n",
    "    def interp_df(interp_tensor: torch.Tensor,\n",
    "                  df_columns: list):\n",
    "        interp_tensor = to_percentage(interp_tensor)\n",
    "\n",
    "        interp = {var: float(val) for var, val in zip(df_columns,\n",
    "                                                      interp_tensor)}\n",
    "        interp = pd.DataFrame(interp, index=[0])\n",
    "\n",
    "        return interp\n",
    "\n",
    "    # find most informative days\n",
    "    _, indices = interp_dict['attention'].sort(descending=True)\n",
    "    indices = indices[0]+1\n",
    "    # get most valuable static parameters\n",
    "    static_worth = interp_df(interp_tensor=interp_dict['static_variables'],\n",
    "                             df_columns=static_parameters)\n",
    "    stat_cols, _ = (list(static_worth.T.nlargest(n=4, columns=0).T.columns),\n",
    "                   list(static_worth.T.nlargest(n=4, columns=0).T.to_numpy()))\n",
    "    \n",
    "    # get most valuable encoder parameters\n",
    "    encoder_worth = interp_df(interp_tensor=interp_dict['encoder_variables'],\n",
    "                              df_columns=encoder_params)\n",
    "    enc_col, _ = (encoder_worth.idxmax(axis=1)[0],\n",
    "                  encoder_worth.max(axis=1)[0])\n",
    "    # get most valuable decoder parameters\n",
    "    decoder_worth = interp_df(interp_tensor=interp_dict['decoder_variables'],\n",
    "                              df_columns=decoder_params)\n",
    "    dec_col, _ = (decoder_worth.idxmax(axis=1)[0],\n",
    "                  decoder_worth.max(axis=1)[0])\n",
    "\n",
    "    return int(indices), stat_cols, enc_col, dec_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_for_tft(\n",
    "        nc_files=[nc_file],\n",
    "        static_path='../geo_data/attributes/geo_vector.csv',\n",
    "        area_index=ws_file.index,\n",
    "        meteo_predictors=meteo_input,\n",
    "        hydro_target=hydro_target, allow_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_gauge_res = list()\n",
    "for nc_file in glob.glob('../geo_data/great_db/nc_all_q/*.nc'):\n",
    "    gauge_id = nc_file.split('/')[-1][:-3]\n",
    "\n",
    "    file = open_for_tft(\n",
    "        nc_files=[nc_file],\n",
    "        static_path='../geo_data/attributes/geo_vector.csv',\n",
    "        area_index=ws_file.index,\n",
    "        meteo_predictors=meteo_input,\n",
    "        hydro_target=hydro_target)\n",
    "\n",
    "    (train_ds, train_loader,\n",
    "        val_ds, val_loader, val_df,\n",
    "        scaler) = train_val_split(file)\n",
    "\n",
    "    # configure network and trainer\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                        min_delta=1e-3, patience=3,\n",
    "                                        verbose=True, mode=\"min\")\n",
    "    # log the learning rate\n",
    "    lr_logger = LearningRateMonitor()\n",
    "    # logging results to a tensorboard\n",
    "    logger = TensorBoardLogger(f\"./single_gauge_8epoch/{gauge_id}_tft\")\n",
    "\n",
    "    if device == 'cuda':\n",
    "        accel = 'gpu'\n",
    "    else:\n",
    "        accel = 'cpu'\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=8,\n",
    "        accelerator='auto',\n",
    "        enable_model_summary=True,\n",
    "        check_val_every_n_epoch=3,\n",
    "        gradient_clip_val=0.5,\n",
    "        log_every_n_steps=3,\n",
    "        callbacks=[lr_logger, early_stop_callback],\n",
    "        logger=logger)\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        train_ds,\n",
    "        learning_rate=1e-3,\n",
    "        hidden_size=64,\n",
    "        dropout=0.4,\n",
    "        loss=nnse(),\n",
    "        optimizer='adam')\n",
    "\n",
    "    # print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "    # fit network\n",
    "    trainer.fit(tft,\n",
    "                train_dataloaders=train_loader,\n",
    "                val_dataloaders=val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import RMSE\n",
    "\n",
    "class nnse(MultiHorizonMetric):\n",
    "\n",
    "    def loss(self, pred, target):\n",
    "\n",
    "        pred = self.to_prediction(pred)\n",
    "        denom = torch.sum((target-pred)**2)\n",
    "        divsr = torch.sum((target - torch.mean(target)**2))\n",
    "        nse = 1 - torch.div(denom, divsr)\n",
    "        nnse = 1 / (2 - nse)\n",
    "\n",
    "        return nnse\n",
    "\n",
    "\n",
    "def nse(pred, target):\n",
    "    denom = np.sum((target-pred)**2)\n",
    "    divsr = np.sum((target-np.mean(target))**2)\n",
    "    return 1-(denom/divsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_indexes = [i.split('/')[-1][:-3] for i in\n",
    "             glob.glob('../geo_data/great_db/nc_all_q/*.nc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = '../geo_data/great_db/nc_all_q/72519.nc'\n",
    "test_ws = ws_file.loc[ws_file['name_ru'] == 'р.Ситня - д.Пески']\n",
    "meteo_input = ['prcp_e5l',  't_max_e5l', 't_min_e5l']\n",
    "hydro_target = 'q_mm_day'\n",
    "static_parameters = ['for_pc_sse', 'crp_pc_sse',\n",
    "                     'inu_pc_ult', 'ire_pc_sse',\n",
    "                     'lka_pc_use', 'prm_pc_sse',\n",
    "                     'pst_pc_sse', 'cly_pc_sav',\n",
    "                     'slt_pc_sav', 'snd_pc_sav',\n",
    "                     'kar_pc_sse', 'urb_pc_sse',\n",
    "                     'gwt_cm_sav', 'lkv_mc_usu',\n",
    "                     'rev_mc_usu', 'sgr_dk_sav',\n",
    "                     'slp_dg_sav', 'ws_area',\n",
    "                     'ele_mt_sav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_id = nc_file.split('/')[-1][:-3]\n",
    "file = open_for_tft(\n",
    "    nc_files=[nc_file],\n",
    "    static_path='../geo_data/attributes/geo_vector.csv',\n",
    "    area_index=test_ws.index,\n",
    "    meteo_predictors=meteo_input,\n",
    "    hydro_target=hydro_target, allow_nan=True)\n",
    "\n",
    "(train_ds, train_loader,\n",
    "    val_ds, val_loader, val_df,\n",
    "    scaler) = train_val_split(file)\n",
    "\n",
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    min_delta=1e-3, patience=3,\n",
    "                                    verbose=True, mode=\"min\")\n",
    "# log the learning rate\n",
    "lr_logger = LearningRateMonitor()\n",
    "# logging results to a tensorboard\n",
    "logger = TensorBoardLogger(f\"./single_gauge/{gauge_id}_tft\")\n",
    "\n",
    "if device == 'cuda':\n",
    "    accel = 'gpu'\n",
    "else:\n",
    "    accel = 'cpu'\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator='auto',\n",
    "    enable_model_summary=True,\n",
    "    check_val_every_n_epoch=3,\n",
    "    gradient_clip_val=0.5,\n",
    "    log_every_n_steps=3,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger)\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_ds,\n",
    "    learning_rate=1e-3,\n",
    "    hidden_size=64,\n",
    "    dropout=0.4,\n",
    "    loss=nnse(),\n",
    "    optimizer='adam')\n",
    "\n",
    "# print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# fit network\n",
    "# trainer.fit(tft,\n",
    "#             train_dataloaders=train_loader,\n",
    "#             val_dataloaders=val_loader)\n",
    "chkpt = glob.glob(\n",
    "    f'./single_gauge/{gauge_id}_tft/*/*/checkpoints/*.ckpt')[0]\n",
    "resdf, interpretation = pred_res_builder(gauge_id=gauge_id,\n",
    "                                         hydro_target=hydro_target,\n",
    "                                         meteo_input=meteo_input,\n",
    "                                         static_parameters=static_parameters,\n",
    "                                         model_checkpoint=chkpt,\n",
    "                                         res_storage='./result/tft_single',\n",
    "                                         val_df=val_df,\n",
    "                                         scaler=scaler,\n",
    "                                         val_ts_ds=val_ds, with_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer.load_from_checkpoint('./TFT_914/lightning_logs/version_0/checkpoints/epoch=2-step=78123.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable selection\n",
    "def make_selection_plot(title, values, labels, ax):\n",
    "    order = np.argsort(values)\n",
    "    values = values / values.sum(-1).unsqueeze(-1)\n",
    "    ax.barh(np.arange(len(values)), values[order] * 100, tick_label=np.asarray(labels)[order])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Значимость в %\")\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(15, 8),\n",
    "                        ncols=2,\n",
    "                        nrows=2)\n",
    "# attention\n",
    "attention = interpretation[\"attention\"].detach().cpu()\n",
    "attention = attention / attention.sum(-1).unsqueeze(-1)\n",
    "axs[0, 0].plot(\n",
    "    np.arange(0, 365), attention\n",
    ")\n",
    "axs[0, 0].set_xlabel(\"Дней назад\")\n",
    "axs[0, 0].set_ylabel(\"Значимость\")\n",
    "axs[0, 0].set_title(\"Значимость\")\n",
    "\n",
    "\n",
    "make_selection_plot(\n",
    "    \"Значимость физико-географических характеристик\", interpretation[\"static_variables\"].detach().cpu(), static_parameters,\n",
    "    ax=axs[0, 1]);\n",
    "make_selection_plot(\n",
    "    \"Значимость переменных кодировщика\",\n",
    "    interpretation[\"encoder_variables\"].detach().cpu(),\n",
    "    [*meteo_input, hydro_target],\n",
    "    ax=axs[1, 0])\n",
    "make_selection_plot(\n",
    "    \"Значимость переменных декодировщика\",\n",
    "    interpretation[\"decoder_variables\"].detach().cpu(),\n",
    "    meteo_input,\n",
    "    ax=axs[1, 1]);\n",
    "fig.savefig('../conclusions/images/interp_model.png',\n",
    "            dpi=650, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = {}\n",
    "# attention\n",
    "fig, ax = plt.subplots()\n",
    "attention = interpretation[\"attention\"].detach().cpu()\n",
    "attention = attention / attention.sum(-1).unsqueeze(-1)\n",
    "ax.plot(\n",
    "    np.arange(0, 365), attention\n",
    ")\n",
    "ax.set_xlabel(\"Дней назад\")\n",
    "ax.set_ylabel(\"Значимость\")\n",
    "ax.set_title(\"Значимость\")\n",
    "figs[\"attention\"] = fig\n",
    "\n",
    "# variable selection\n",
    "def make_selection_plot(title, values, labels):\n",
    "    fig, ax = plt.subplots(figsize=(7, len(values) * 0.25 + 2))\n",
    "    order = np.argsort(values)\n",
    "    values = values / values.sum(-1).unsqueeze(-1)\n",
    "    ax.barh(np.arange(len(values)), values[order] * 100, tick_label=np.asarray(labels)[order])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Значимость в %\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "figs[\"static_variables\"] = make_selection_plot(\n",
    "    \"Значимость физико-географических характеристик\", interpretation[\"static_variables\"].detach().cpu(), static_parameters\n",
    ")\n",
    "figs[\"encoder_variables\"] = make_selection_plot(\n",
    "    \"Значимость переменных кодировщика\", interpretation[\"encoder_variables\"].detach().cpu(), [*meteo_input, hydro_target],\n",
    ")\n",
    "figs[\"decoder_variables\"] = make_selection_plot(\n",
    "    \"Значимость переменных декодировщика\", interpretation[\"decoder_variables\"].detach().cpu(), meteo_input\n",
    ")\n",
    "fig, axs = plt.subplots(figsize=(15, 8),\n",
    "                        ncols=2,\n",
    "                        nrows=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_gauges = gpd.read_file(\n",
    "    '../geo_data/great_db/geometry/gauges_partial_q.gpkg')\n",
    "partial_gauges.index = partial_gauges['gauge_id'].astype(str)\n",
    "partial_ws = ws_file[ws_file.index.isin(partial_gauges.index)]\n",
    "partial_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_gauges = gpd.read_file('../geo_data/great_db/geometry/lost_gauges.gpkg')\n",
    "lost_gauges.index = lost_gauges['gauge_id'].astype(str)\n",
    "lost_ws = ws_file[ws_file.index.isin(lost_gauges.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_input = ['prcp_e5l',  't_max_e5l', 't_min_e5l']\n",
    "hydro_target = 'q_mm_day'\n",
    "static_parameters = ['for_pc_sse', 'crp_pc_sse',\n",
    "                     'inu_pc_ult', 'ire_pc_sse',\n",
    "                     'lka_pc_use', 'prm_pc_sse',\n",
    "                     'pst_pc_sse', 'cly_pc_sav',\n",
    "                     'slt_pc_sav', 'snd_pc_sav',\n",
    "                     'kar_pc_sse', 'urb_pc_sse',\n",
    "                     'gwt_cm_sav', 'lkv_mc_usu',\n",
    "                     'rev_mc_usu', 'sgr_dk_sav',\n",
    "                     'slp_dg_sav', 'ws_area',\n",
    "                     'ele_mt_sav']\n",
    "res_list = list()\n",
    "for nc_file in [file for file\n",
    "                in glob.glob('../geo_data/great_db/nc_concat/*.nc')\n",
    "                if file.split('/')[-1][:-3] in lost_ws.index]:\n",
    "    gauge_id = nc_file.split('/')[-1][:-3]\n",
    "    file = open_for_tft(\n",
    "        nc_files=[nc_file],\n",
    "        static_path='../geo_data/attributes/geo_vector.csv',\n",
    "        area_index=lost_ws.index,\n",
    "        meteo_predictors=meteo_input,\n",
    "        hydro_target=hydro_target, allow_nan=True)\n",
    "    try:\n",
    "        (train_ds, train_loader,\n",
    "         val_ds, val_loader, val_df,\n",
    "         scaler) = train_val_split(file)\n",
    "\n",
    "        res_df, interpretation = pred_res_builder(gauge_id=gauge_id,\n",
    "                                                  hydro_target=hydro_target,\n",
    "                                                  meteo_input=meteo_input,\n",
    "                                                  static_parameters=static_parameters,\n",
    "                                                  model_checkpoint='/workspaces/my_dissertation/forecast/TFT_914/lightning_logs/version_0/checkpoints/epoch=2-step=78123.ckpt',\n",
    "                                                  res_storage='./result/lost_gauge',\n",
    "                                                  val_df=val_df,\n",
    "                                                  scaler=scaler,\n",
    "                                                  val_ts_ds=val_ds, with_plot=False)\n",
    "        res_list.append(res_df)\n",
    "    except AssertionError:\n",
    "        print(f'No available data for {gauge_id}')\n",
    "pd.concat(res_list).to_csv('./result/tft_lost_gauge.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blind forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_input = ['prcp_e5l',  't_max_e5l', 't_min_e5l']\n",
    "hydro_target = 'q_mm_day'\n",
    "static_parameters = ['for_pc_sse', 'crp_pc_sse',\n",
    "                     'inu_pc_ult', 'ire_pc_sse',\n",
    "                     'lka_pc_use', 'prm_pc_sse',\n",
    "                     'pst_pc_sse', 'cly_pc_sav',\n",
    "                     'slt_pc_sav', 'snd_pc_sav',\n",
    "                     'kar_pc_sse', 'urb_pc_sse',\n",
    "                     'gwt_cm_sav', 'lkv_mc_usu',\n",
    "                     'rev_mc_usu', 'sgr_dk_sav',\n",
    "                     'slp_dg_sav', 'ws_area',\n",
    "                     'ele_mt_sav']\n",
    "\n",
    "partial_gauges = gpd.read_file(\n",
    "    '../geo_data/great_db/geometry/gauges_partial_q.gpkg')\n",
    "partial_gauges.index = partial_gauges['gauge_id'].astype(str)\n",
    "partial_ws = ws_file[ws_file.index.isin(partial_gauges.index)]\n",
    "\n",
    "res_list = list()\n",
    "for nc_file in [file for file\n",
    "                in glob.glob('../geo_data/great_db/nc_concat/*.nc')\n",
    "                if file.split('/')[-1][:-3] in partial_ws.index]:\n",
    "    gauge_id = nc_file.split('/')[-1][:-3]\n",
    "    file = open_for_tft(\n",
    "        nc_files=[nc_file],\n",
    "        static_path='../geo_data/attributes/geo_vector.csv',\n",
    "        area_index=partial_ws.index,\n",
    "        meteo_predictors=meteo_input,\n",
    "        hydro_target=hydro_target, allow_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_for_tft(\n",
    "        nc_files=[nc_file],\n",
    "        static_path='../geo_data/attributes/geo_vector.csv',\n",
    "        area_index=partial_ws.index,\n",
    "        meteo_predictors=meteo_input,\n",
    "        hydro_target=hydro_target, allow_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_attributes = pd.read_csv('../geo_data/attributes/geo_vector.csv',\n",
    "                                index_col='gauge_id')\n",
    "static_attributes.index = static_attributes.index.astype(str)\n",
    "static_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_attributes.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_attributes = pd.read_csv('../geo_data/attributes/geo_vector.csv',\n",
    "                                index_col='gauge_id')\n",
    "static_attributes.index = static_attributes.index.astype(str)\n",
    "gauge_id = nc_file.split('/')[-1][:-3]\n",
    "res_file = list()\n",
    "try:\n",
    "    gauge_static = static_attributes.loc[[gauge_id], :]\n",
    "except KeyError:\n",
    "    print(f'No data for {gauge_id} !')\n",
    "\n",
    "else:\n",
    "    file = xr.open_dataset(nc_file)\n",
    "    file = file.to_dataframe()\n",
    "    # file['date'] = file.index\n",
    "    file = file.reset_index()\n",
    "    file['time_idx'] = file.index\n",
    "\n",
    "    for col in gauge_static.columns:\n",
    "        file[col] = gauge_static.loc[gauge_id, col]\n",
    "\n",
    "    res_file.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open_for_tft(\n",
    "        nc_files=[nc_file],\n",
    "        static_path='../geo_data/attributes/geo_vector.csv',\n",
    "        area_index=lost_ws.index,\n",
    "        meteo_predictors=meteo_input,\n",
    "        hydro_target=hydro_target, allow_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'index' in f.columns:\n",
    "    f = f.rename(columns={'index': 'date'})\n",
    "f = f[['date', 'time_idx', 'gauge_id',\n",
    "                    hydro_target, *meteo_input, *static_parameters]]\n",
    "f = f.dropna().reset_index(drop=True)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(1, 10))\n",
    "f[[hydro_target, *meteo_input,\n",
    "        *static_parameters]] = scaler.fit_transform(\n",
    "    f[[hydro_target, *meteo_input, *static_parameters]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this to get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import pytorch_lightning as pl\n",
    "from scripts.tft_data import open_for_tft, train_val_split\n",
    "from scripts.model_eval import nnse, pred_res_builder\n",
    "\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "# Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3, 1), 'GB')\n",
    "\n",
    "\n",
    "meteo_input = ['prcp_e5l',  't_max_e5l', 't_min_e5l']\n",
    "hydro_target = 'lvl_mbs'\n",
    "static_parameters = ['for_pc_sse', 'crp_pc_sse',\n",
    "                     'inu_pc_ult', 'ire_pc_sse',\n",
    "                     'lka_pc_use', 'prm_pc_sse',\n",
    "                     'pst_pc_sse', 'cly_pc_sav',\n",
    "                     'slt_pc_sav', 'snd_pc_sav',\n",
    "                     'kar_pc_sse', 'urb_pc_sse',\n",
    "                     'gwt_cm_sav', 'lkv_mc_usu',\n",
    "                     'rev_mc_usu', 'sgr_dk_sav',\n",
    "                     'slp_dg_sav', 'ws_area',\n",
    "                     'ele_mt_sav']\n",
    "\n",
    "ws_file = gpd.read_file('../geo_data/great_db/geometry/russia_ws.gpkg')\n",
    "ws_file = ws_file.set_index('gauge_id')\n",
    "# ws_file = ws_file[ws_file['new_area'] <= 50000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "file = open_for_tft(\n",
    "    nc_files=glob.glob('../geo_data/great_db/nc_all_h/*.nc'),\n",
    "    static_path='../geo_data/attributes/geo_vector.csv',\n",
    "    area_index=ws_file.index,\n",
    "    meteo_input=meteo_input,\n",
    "    hydro_target=hydro_target,\n",
    "    shuffle_static=False,\n",
    "    with_static=True)\n",
    "\n",
    "(train_ds, train_loader,\n",
    "    val_ds, val_loader, val_df,\n",
    "    scaler) = train_val_split(file, with_static=True)\n",
    "res = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gauge in val_df.gauge_id.unique():\n",
    "    try:\n",
    "        res.append(pred_res_builder(gauge_id=gauge,\n",
    "                                    res_storage='./result/tft_level_multi_256/',\n",
    "                                    model_checkpoint='/workspaces/my_dissertation/forecast/lvl_prediction_multi_gauge_NEXT/lightning_logs/version_0/checkpoints/epoch=11-step=200928.ckpt',\n",
    "                                    hydro_target=hydro_target,\n",
    "                                    meteo_input=meteo_input,\n",
    "                                    static_parameters=static_parameters,\n",
    "                                    val_ts_ds=val_ds, val_df=val_df,\n",
    "                                    scaler=scaler,\n",
    "                                    with_plot=False)[0])\n",
    "    except:\n",
    "        pass\n",
    "res = pd.concat(res)\n",
    "res.to_csv('./result/tft_level_multi_256.csv',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./result/tft_shuffled_static_pred_17epoch_RMSE_256.csv')['NSE'].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(\n",
    "    '/workspaces/my_dissertation/forecast/TFT_914/lightning_logs/version_6/checkpoints/epoch=20-step=246057.ckpt')\n",
    "res = list()\n",
    "for gauge in tqdm(val_df.gauge_id.unique()):\n",
    "    res.append(pred_res_builder(gauge_id=gauge,\n",
    "                                val_ts_ds=validation,\n",
    "                                with_plot=False))\n",
    "res = pd.concat(res)\n",
    "res.to_csv('./result/tft_predictions_6.csv',\n",
    "           index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(\n",
    "    '/workspaces/my_dissertation/forecast/multi_gauge_256/lightning_logs/version_0/checkpoints/epoch=20-step=274596.ckpt')\n",
    "\n",
    "\n",
    "pred_id = '5461'\n",
    "file = open_for_tft(\n",
    "    nc_files=glob.glob(f'../geo_data/great_db/nc_all_q/{pred_id}.nc'),\n",
    "    static_path='../geo_data/attributes/geo_vector.csv',\n",
    "    area_index=ws_file.index,\n",
    "    meteo_predictors=meteo_input,\n",
    "    hydro_target=hydro_target)\n",
    "\n",
    "(train_ds, train_loader,\n",
    "    val_ds, val_loader, val_df,\n",
    "    scaler) = train_val_split(file)\n",
    "\n",
    "raw_predictions, _, x, _, _ = best_tft.predict(val_ds.filter(\n",
    "    lambda x: x.gauge_id == f'{pred_id}').to_dataloader(train=False,\n",
    "                                                   batch_size=128,\n",
    "                                                   num_workers=8), mode=\"raw\",\n",
    "    return_x=True)\n",
    "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
