{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92994cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "from neuralhydrology.evaluation import get_tester\n",
    "from neuralhydrology.utils.config import Config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.readers.geom_reader import load_geodata\n",
    "from src.timeseries_stats.metrics import evaluate_model\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "LOG = setup_logger(\"fine_tune\", log_file=\"../logs/fine_tuning.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97ed4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 996 gauges with hybrid classification\n"
     ]
    }
   ],
   "source": [
    "# Load watershed geometries and gauge locations\n",
    "ws, gauges = load_geodata(folder_depth=\"../\")\n",
    "common_index = gauges.index.to_list()\n",
    "basemap_data = gpd.read_file(\"../data/geometry/basemap_2023.gpkg\")\n",
    "# Load cluster assignments (from Chapter 1)\n",
    "# gauge_mapping = pd.read_csv(\n",
    "#     \"../res/chapter_one/gauge_hybrid_mapping.csv\",\n",
    "#     index_col=\"gauge_id\",\n",
    "#     dtype={\"gauge_id\": str},\n",
    "# )\n",
    "\n",
    "print(f\"Loaded {len(gauges)} gauges with hybrid classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecabc989",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_gauges = gpd.read_file(\"../res/FineTuneGauges.gpkg\")[\n",
    "    [\n",
    "        \"gauge_id\",\n",
    "        \"name_ru\",\n",
    "        \"name_en\",\n",
    "        \"geometry\",\n",
    "        \"lstm_nse_mswep\",\n",
    "        \"lstm_nse_e5l\",\n",
    "        \"lstm_nse_e5\",\n",
    "        \"lstm_nse_gpcp\",\n",
    "    ]\n",
    "]\n",
    "fine_tune_gauges.set_index(\"gauge_id\", inplace=True)\n",
    "\n",
    "ft_index = fine_tune_gauges.index.tolist()\n",
    "rest_gauges = gauges.loc[~gauges.index.isin(ft_index)]\n",
    "rest_index = rest_gauges.index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fe225",
   "metadata": {},
   "source": [
    "### Draw predictions vs observations for fine-tuned gauges (before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00262dd9",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2025-12-12 16:11:34 | INFO     | PhDLogger | fine_tune | ℹ️  Initial parameters for fine-tuning gauges: \u001b[0m\n",
      "\u001b[38;5;39m2025-12-12 16:11:34 | INFO     | PhDLogger | fine_tune | ℹ️  MSWEP NSE: 0.25\u001b[0m\n",
      "\u001b[38;5;39m2025-12-12 16:11:34 | INFO     | PhDLogger | fine_tune | ℹ️  E5L NSE: 0.17\u001b[0m\n",
      "\u001b[38;5;39m2025-12-12 16:11:34 | INFO     | PhDLogger | fine_tune | ℹ️  E5 NSE: 0.12\u001b[0m\n",
      "\u001b[38;5;39m2025-12-12 16:11:34 | INFO     | PhDLogger | fine_tune | ℹ️  GPCP NSE: 0.16\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "LOG.info(\n",
    "    \"Initial parameters for fine-tuning gauges: \",\n",
    ")\n",
    "LOG.info(\"MSWEP NSE: %.2f\", fine_tune_gauges[\"lstm_nse_mswep\"].median())\n",
    "LOG.info(\"E5L NSE: %.2f\", fine_tune_gauges[\"lstm_nse_e5l\"].median())\n",
    "LOG.info(\"E5 NSE: %.2f\", fine_tune_gauges[\"lstm_nse_e5\"].median())\n",
    "LOG.info(\"GPCP NSE: %.2f\", fine_tune_gauges[\"lstm_nse_gpcp\"].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "917de0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pathes = {\n",
    "    \"gpcp\": \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_gpcp_no_autocorr_static_1203_080402/test/model_epoch024/test_results.p\",\n",
    "    \"e5\": \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_era5_no_autocorr_static_1203_220232/test/model_epoch020/test_results.p\",\n",
    "    \"e5l\": \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_era5l_no_autocorr_static_1003_133332/test/model_epoch026/test_results.p\",\n",
    "    \"mswep\": \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_mswep_no_autocorr_static_1103_191754/test/model_epoch024/test_results.p\",\n",
    "    \"ealstm\": \"../data/lstm_configs/model_runs/ealstm_q_mm_day_mswep_no_static_0205_144958/test/model_epoch038/test_results.p\",\n",
    "}\n",
    "\n",
    "cfg_pathes = {\n",
    "    \"gpcp\": {\n",
    "        \"path\": Path(\n",
    "            \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_gpcp_no_autocorr_static_1203_080402/config.yml\"\n",
    "        ),\n",
    "        \"epoch\": 24,\n",
    "    },\n",
    "    \"mswep\": {\n",
    "        \"path\": Path(\n",
    "            \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_mswep_no_autocorr_static_1103_191754/config.yml\"\n",
    "        ),\n",
    "        \"epoch\": 24,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"../data/models/fine_tune/poor_gauges.txt\", \"w\") as the_file:\n",
    "    for gauge_name in ft_index:\n",
    "        the_file.write(f\"{int(gauge_name)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "752b8070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 265/265 [01:02<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "lstm_cfg = cfg_pathes[\"mswep\"][\"path\"]\n",
    "epoch = cfg_pathes[\"mswep\"][\"epoch\"]\n",
    "\n",
    "cfg_run = Config(lstm_cfg)\n",
    "\n",
    "cfg_run.update_config(\n",
    "    {\n",
    "        \"train_basin_file\": \"../data/models/fine_tune/poor_gauges.txt\",\n",
    "        \"validate_n_random_basins\": len(ft_index),\n",
    "        \"validation_basin_file\": \"../data/models/fine_tune/poor_gauges.txt\",\n",
    "        \"test_basin_file\": \"../data/models/fine_tune/poor_gauges.txt\",\n",
    "        \"test_start_date\": \"01/01/2009\",\n",
    "        \"test_end_date\": \"31/12/2020\",\n",
    "    }\n",
    ")\n",
    "tester = get_tester(cfg=cfg_run, run_dir=cfg_run.run_dir, period=\"test\", init_model=True)\n",
    "pred_results = tester.evaluate(epoch=epoch, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ea339",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_nse_data = []\n",
    "\n",
    "print(\"Calculating yearly and period NSE for GPCP simulation results...\")\n",
    "\n",
    "if not pred_results:\n",
    "    print(\n",
    "        \"No prediction results found. Please ensure the evaluation cell above ran successfully.\"\n",
    "    )\n",
    "else:\n",
    "    for gauge_id, result in pred_results.items():\n",
    "        # Extract data from NeuralHydrology result\n",
    "        # Assuming structure matches previous cells: result[\"1D\"][\"xr\"] is an xarray Dataset\n",
    "        try:\n",
    "            df = result[\"1D\"][\"xr\"].to_dataframe().loc[\"2009\":,]\n",
    "        except KeyError:\n",
    "            print(f\"No data found for gauge {gauge_id} in the specified period.\")\n",
    "            continue\n",
    "\n",
    "        # Handle multi-index if present (usually it's date/basin)\n",
    "        if isinstance(df.index, pd.MultiIndex):\n",
    "            df = df.droplevel(1)\n",
    "\n",
    "        # Group by year\n",
    "        for year, group in df.groupby(df.index.year):\n",
    "            obs = group[\"q_mm_day_obs\"]\n",
    "            sim = group[\"q_mm_day_sim\"]\n",
    "\n",
    "            # Calculate NSE if we have enough data\n",
    "            if len(obs.dropna()) > 10:\n",
    "                try:\n",
    "                    metrics = evaluate_model(observed=obs, simulated=sim)\n",
    "                    nse = metrics[\"NSE\"]\n",
    "                except Exception:\n",
    "                    nse = np.nan\n",
    "            else:\n",
    "                nse = np.nan\n",
    "\n",
    "            yearly_nse_data.append({\"gauge_id\": gauge_id, \"period\": year, \"nse\": nse})\n",
    "\n",
    "        # Calculate specific periods\n",
    "        periods = {\"2009-2018\": df.loc[\"2009\":\"2018\"], \"2019-2020\": df.loc[\"2019\":\"2020\"]}\n",
    "\n",
    "        for p_name, p_df in periods.items():\n",
    "            nse = np.nan\n",
    "            if len(p_df) > 0:\n",
    "                obs = p_df[\"q_mm_day_obs\"]\n",
    "                sim = p_df[\"q_mm_day_sim\"]\n",
    "                if len(obs.dropna()) > 10:\n",
    "                    try:\n",
    "                        metrics = evaluate_model(observed=obs, simulated=sim)\n",
    "                        nse = metrics[\"NSE\"]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            yearly_nse_data.append({\"gauge_id\": gauge_id, \"period\": p_name, \"nse\": nse})\n",
    "\n",
    "    if not yearly_nse_data:\n",
    "        print(\"No NSE data calculated.\")\n",
    "    else:\n",
    "        # Create pivot table: Rows=Gauge, Cols=Period\n",
    "        nse_df = pd.DataFrame(yearly_nse_data)\n",
    "\n",
    "        # Ensure 'period' column exists even if empty (though yearly_nse_data check handles empty list)\n",
    "        if \"period\" not in nse_df.columns:\n",
    "            print(\"Error: 'period' column missing from data.\")\n",
    "        else:\n",
    "            nse_pivot = nse_df.pivot(index=\"gauge_id\", columns=\"period\", values=\"nse\")\n",
    "\n",
    "            # Reorder columns: Years first, then periods\n",
    "            year_cols = sorted([c for c in nse_pivot.columns if isinstance(c, int)])\n",
    "            period_cols = [\"2009-2018\", \"2019-2020\"]\n",
    "            # Ensure period columns exist (in case of missing data)\n",
    "            period_cols = [c for c in period_cols if c in nse_pivot.columns]\n",
    "            nse_pivot = nse_pivot[year_cols + period_cols]\n",
    "\n",
    "            # Display with heatmap styling\n",
    "            # vmin=-1 to handle poor performance without skewing the color scale too much for \"okay\" values\n",
    "            styled_df = nse_pivot.style.background_gradient(\n",
    "                cmap=\"RdYlGn\", vmin=0, vmax=1\n",
    "            ).format(\"{:.2f}\")\n",
    "            # display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4431fbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "period\n",
       "2009-2018    0.724910\n",
       "2019-2020    0.251876\n",
       "dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nse_pivot[period_cols].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c9251",
   "metadata": {},
   "source": [
    "### Rest gauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4290ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 731/731 [02:43<00:00,  4.46it/s]\n"
     ]
    }
   ],
   "source": [
    "lstm_pathes = {\n",
    "    \"gpcp\": \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_gpcp_no_autocorr_static_1203_080402/test/model_epoch024/test_results.p\",\n",
    "    \"e5\": \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_era5_no_autocorr_static_1203_220232/test/model_epoch020/test_results.p\",\n",
    "    \"e5l\": \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_era5l_no_autocorr_static_1003_133332/test/model_epoch026/test_results.p\",\n",
    "    \"mswep\": \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_mswep_no_autocorr_static_1103_191754/test/model_epoch024/test_results.p\",\n",
    "    \"ealstm\": \"../data/lstm_configs/model_runs/ealstm_q_mm_day_mswep_no_static_0205_144958/test/model_epoch038/test_results.p\",\n",
    "}\n",
    "\n",
    "cfg_pathes = {\n",
    "    \"gpcp\": {\n",
    "        \"path\": Path(\n",
    "            \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_gpcp_no_autocorr_static_1203_080402/config.yml\"\n",
    "        ),\n",
    "        \"epoch\": 24,\n",
    "    },\n",
    "    \"mswep\": {\n",
    "        \"path\": Path(\n",
    "            \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_mswep_no_autocorr_static_1103_191754/config.yml\"\n",
    "        ),\n",
    "        \"epoch\": 24,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"../data/models/fine_tune/rest_gauges.txt\", \"w\") as the_file:\n",
    "    for gauge_name in rest_index:\n",
    "        the_file.write(f\"{int(gauge_name)}\\n\")\n",
    "lstm_cfg = cfg_pathes[\"mswep\"][\"path\"]\n",
    "epoch = cfg_pathes[\"mswep\"][\"epoch\"]\n",
    "\n",
    "cfg_run = Config(lstm_cfg)\n",
    "\n",
    "cfg_run.update_config(\n",
    "    {\n",
    "        \"train_basin_file\": \"../data/models/fine_tune/rest_gauges.txt\",\n",
    "        \"validate_n_random_basins\": len(ft_index),\n",
    "        \"validation_basin_file\": \"../data/models/fine_tune/rest_gauges.txt\",\n",
    "        \"test_basin_file\": \"../data/models/fine_tune/rest_gauges.txt\",\n",
    "        \"test_start_date\": \"01/01/2009\",\n",
    "        \"test_end_date\": \"31/12/2020\",\n",
    "    }\n",
    ")\n",
    "tester = get_tester(cfg=cfg_run, run_dir=cfg_run.run_dir, period=\"test\", init_model=True)\n",
    "pred_results = tester.evaluate(epoch=epoch, save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "005ff068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating yearly and period NSE for GPCP simulation results...\n"
     ]
    }
   ],
   "source": [
    "yearly_nse_data = []\n",
    "\n",
    "print(\"Calculating yearly and period NSE for GPCP simulation results...\")\n",
    "\n",
    "if not pred_results:\n",
    "    print(\n",
    "        \"No prediction results found. Please ensure the evaluation cell above ran successfully.\"\n",
    "    )\n",
    "else:\n",
    "    for gauge_id, result in pred_results.items():\n",
    "        # Extract data from NeuralHydrology result\n",
    "        # Assuming structure matches previous cells: result[\"1D\"][\"xr\"] is an xarray Dataset\n",
    "        try:\n",
    "            df = result[\"1D\"][\"xr\"].to_dataframe().loc[\"2009\":,]\n",
    "        except KeyError:\n",
    "            print(f\"No data found for gauge {gauge_id} in the specified period.\")\n",
    "            continue\n",
    "\n",
    "        # Handle multi-index if present (usually it's date/basin)\n",
    "        if isinstance(df.index, pd.MultiIndex):\n",
    "            df = df.droplevel(1)\n",
    "\n",
    "        # Group by year\n",
    "        for year, group in df.groupby(df.index.year):\n",
    "            obs = group[\"q_mm_day_obs\"]\n",
    "            sim = group[\"q_mm_day_sim\"]\n",
    "\n",
    "            # Calculate NSE if we have enough data\n",
    "            if len(obs.dropna()) > 10:\n",
    "                try:\n",
    "                    metrics = evaluate_model(observed=obs, simulated=sim)\n",
    "                    nse = metrics[\"NSE\"]\n",
    "                except Exception:\n",
    "                    nse = np.nan\n",
    "            else:\n",
    "                nse = np.nan\n",
    "\n",
    "            yearly_nse_data.append({\"gauge_id\": gauge_id, \"period\": year, \"nse\": nse})\n",
    "\n",
    "        # Calculate specific periods\n",
    "        periods = {\"2009-2018\": df.loc[\"2009\":\"2018\"], \"2019-2020\": df.loc[\"2019\":\"2020\"]}\n",
    "\n",
    "        for p_name, p_df in periods.items():\n",
    "            nse = np.nan\n",
    "            if len(p_df) > 0:\n",
    "                obs = p_df[\"q_mm_day_obs\"]\n",
    "                sim = p_df[\"q_mm_day_sim\"]\n",
    "                if len(obs.dropna()) > 10:\n",
    "                    try:\n",
    "                        metrics = evaluate_model(observed=obs, simulated=sim)\n",
    "                        nse = metrics[\"NSE\"]\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            yearly_nse_data.append({\"gauge_id\": gauge_id, \"period\": p_name, \"nse\": nse})\n",
    "\n",
    "    if not yearly_nse_data:\n",
    "        print(\"No NSE data calculated.\")\n",
    "    else:\n",
    "        # Create pivot table: Rows=Gauge, Cols=Period\n",
    "        nse_df = pd.DataFrame(yearly_nse_data)\n",
    "\n",
    "        # Ensure 'period' column exists even if empty (though yearly_nse_data check handles empty list)\n",
    "        if \"period\" not in nse_df.columns:\n",
    "            print(\"Error: 'period' column missing from data.\")\n",
    "        else:\n",
    "            nse_pivot = nse_df.pivot(index=\"gauge_id\", columns=\"period\", values=\"nse\")\n",
    "\n",
    "            # Reorder columns: Years first, then periods\n",
    "            year_cols = sorted([c for c in nse_pivot.columns if isinstance(c, int)])\n",
    "            period_cols = [\"2009-2018\", \"2019-2020\"]\n",
    "            # Ensure period columns exist (in case of missing data)\n",
    "            period_cols = [c for c in period_cols if c in nse_pivot.columns]\n",
    "            nse_pivot = nse_pivot[year_cols + period_cols]\n",
    "\n",
    "            # Display with heatmap styling\n",
    "            # vmin=-1 to handle poor performance without skewing the color scale too much for \"okay\" values\n",
    "            styled_df = nse_pivot.style.background_gradient(\n",
    "                cmap=\"RdYlGn\", vmin=0, vmax=1\n",
    "            ).format(\"{:.2f}\")\n",
    "            # display(styled_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6fade53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "period\n",
       "2009-2018    0.837036\n",
       "2019-2020    0.708212\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nse_pivot[period_cols].median()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532ef9c4",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process LSTM pickle files and create dataframes with comprehensive metrics\n",
    "lstm_dataset_dfs = {}\n",
    "\n",
    "for _dataset_name, _pickle_path in lstm_pathes.items():\n",
    "    print(f\"Processing LSTM {_dataset_name}...\")\n",
    "\n",
    "    with open(_pickle_path, \"rb\") as _f:\n",
    "        _lstm_data = pickle.load(_f)\n",
    "\n",
    "    lstm_dataset_dfs[_dataset_name] = {}\n",
    "\n",
    "    for _gauge_id, _gauge_results in _lstm_data.items():\n",
    "        if _gauge_id in ft_index:\n",
    "            # Extract observed and simulated data\n",
    "            _df_data = (\n",
    "                _gauge_results[\"1D\"][\"xr\"]\n",
    "                .to_dataframe()\n",
    "                .droplevel(1)\n",
    "                .rename(columns={\"q_mm_day_obs\": \"obs\", \"q_mm_day_sim\": \"sim\"})\n",
    "            )\n",
    "            lstm_dataset_dfs[_dataset_name][_gauge_id] = _df_data\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b659761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = Path(\"../data/images/series_before_finetuning\")\n",
    "img_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4b5819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create output directory\n",
    "img_dir = Path(\"../data/images/series_before_finetuning\")\n",
    "img_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get list of gauges from the first dataset (assuming all have same gauges)\n",
    "first_dataset = next(iter(lstm_dataset_dfs.values()))\n",
    "gauge_ids = first_dataset.keys()\n",
    "\n",
    "for gauge_id in gauge_ids:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot observed data (only once)\n",
    "    # We can take observed from any dataset as it should be the same\n",
    "    obs_data = lstm_dataset_dfs[\"mswep\"][gauge_id][\"obs\"]\n",
    "    ax.plot(\n",
    "        obs_data.index,\n",
    "        obs_data,\n",
    "        label=\"Observed\",\n",
    "        color=\"black\",\n",
    "        linewidth=1.5,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "\n",
    "    title_parts = [f\"Gauge {gauge_id}\"]\n",
    "\n",
    "    # Plot simulations for each dataset\n",
    "    colors = {\n",
    "        \"gpcp\": \"red\",\n",
    "        \"mswep\": \"blue\",\n",
    "        \"e5l\": \"green\",\n",
    "        \"e5\": \"orange\",\n",
    "        \"ealstm\": \"purple\",\n",
    "    }\n",
    "\n",
    "    for dataset_name, dataset_dfs in lstm_dataset_dfs.items():\n",
    "        if gauge_id in dataset_dfs:\n",
    "            sim_data = dataset_dfs[gauge_id][\"sim\"]\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = evaluate_model(observed=obs_data, simulated=sim_data)\n",
    "\n",
    "            # Add to plot\n",
    "            ax.plot(\n",
    "                sim_data.index,\n",
    "                sim_data,\n",
    "                label=f\"{dataset_name} (NSE: {metrics['NSE']:.2f})\",\n",
    "                color=colors.get(dataset_name, \"gray\"),\n",
    "                linewidth=1,\n",
    "                alpha=0.6,\n",
    "            )\n",
    "\n",
    "            # Add metrics to title (simplified)\n",
    "            title_parts.append(\n",
    "                f\"{dataset_name}: NSE={metrics['NSE']:.2f}, KGE={metrics['KGE']:.2f}\"\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Discharge (mm/day)\")\n",
    "    ax.set_title(\"\\n\".join([title_parts[0], \" | \".join(title_parts[1:])]), fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Save figure\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(img_dir / f\"gauge_{gauge_id}_comparison.png\", dpi=150)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e793f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1518a098",
   "metadata": {},
   "source": [
    "### Read pre-trained configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b23508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fb597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4f092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b92875b3",
   "metadata": {},
   "source": [
    "### Fine-tune for poor-performing gauges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6dcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd06e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba56733f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135034c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89077357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3277c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c15c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
