{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96064a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/models/full/full_gauges.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tester = \u001b[43mget_tester\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg_run\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m pred_results = tester.evaluate(epoch=epoch, save_results=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m model_results[model] = pred_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd_env/lib/python3.11/site-packages/neuralhydrology/evaluation/__init__.py:35\u001b[39m, in \u001b[36mget_tester\u001b[39m\u001b[34m(cfg, run_dir, period, init_model)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     33\u001b[39m     \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo evaluation method implemented for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg.head\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m head\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTester\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd_env/lib/python3.11/site-packages/neuralhydrology/evaluation/tester.py:525\u001b[39m, in \u001b[36mRegressionTester.__init__\u001b[39m\u001b[34m(self, cfg, run_dir, period, init_model)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: Config, run_dir: Path, period: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m, init_model: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRegressionTester\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd_env/lib/python3.11/site-packages/neuralhydrology/evaluation/tester.py:81\u001b[39m, in \u001b[36mBaseTester.__init__\u001b[39m\u001b[34m(self, cfg, run_dir, period, init_model)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m.loss_obj = get_loss_obj(cfg)\n\u001b[32m     79\u001b[39m \u001b[38;5;28mself\u001b[39m.loss_obj.set_regularization_terms(get_regularization_obj(cfg=\u001b[38;5;28mself\u001b[39m.cfg))\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_run_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd_env/lib/python3.11/site-packages/neuralhydrology/evaluation/tester.py:110\u001b[39m, in \u001b[36mBaseTester._load_run_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load run specific data from run directory\"\"\"\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# get list of basins\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28mself\u001b[39m.basins = \u001b[43mload_basin_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mperiod\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_basin_file\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# load feature scaler\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler = load_scaler(\u001b[38;5;28mself\u001b[39m.run_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd_env/lib/python3.11/site-packages/neuralhydrology/datautils/utils.py:130\u001b[39m, in \u001b[36mload_basin_file\u001b[39m\u001b[34m(basin_file)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_basin_file\u001b[39m(basin_file: Path) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    111\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load list of basins from text file.\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    Note: Basins names are not allowed to end with '_period*'\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[33;03m        In case of invalid basin names that would cause problems internally.\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mbasin_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    131\u001b[39m         basins = \u001b[38;5;28msorted\u001b[39m(basin.strip() \u001b[38;5;28;01mfor\u001b[39;00m basin \u001b[38;5;129;01min\u001b[39;00m fp \u001b[38;5;28;01mif\u001b[39;00m basin.strip())\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# sanity check basin names\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd_env/lib/python3.11/pathlib.py:1044\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1043\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m io.open(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/models/full/full_gauges.txt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3406c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/lstm_configs/launch_configs')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_cfg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca160b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92994cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.readers.geom_reader import load_geodata\n",
    "from src.timeseries_stats.metrics import evaluate_model\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "LOG = setup_logger(\"fine_tune\", log_file=\"../logs/fine_tuning.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97ed4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 996 gauges with hybrid classification\n"
     ]
    }
   ],
   "source": [
    "# Load watershed geometries and gauge locations\n",
    "ws, gauges = load_geodata(folder_depth=\"../\")\n",
    "common_index = gauges.index.to_list()\n",
    "basemap_data = gpd.read_file(\"../data/geometry/basemap_2023.gpkg\")\n",
    "# Load cluster assignments (from Chapter 1)\n",
    "# gauge_mapping = pd.read_csv(\n",
    "#     \"../res/chapter_one/gauge_hybrid_mapping.csv\",\n",
    "#     index_col=\"gauge_id\",\n",
    "#     dtype={\"gauge_id\": str},\n",
    "# )\n",
    "\n",
    "print(f\"Loaded {len(gauges)} gauges with hybrid classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecabc989",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_gauges = gpd.read_file(\"../res/FineTuneGauges.gpkg\")[\n",
    "    [\n",
    "        \"gauge_id\",\n",
    "        \"name_ru\",\n",
    "        \"name_en\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "]\n",
    "fine_tune_gauges.set_index(\"gauge_id\", inplace=True)\n",
    "\n",
    "ft_index = fine_tune_gauges.index.tolist()\n",
    "rest_gauges = gauges.loc[~gauges.index.isin(ft_index)]\n",
    "rest_index = rest_gauges.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06479fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metrics_from_folder(base_path: Path, gauge_id: str, dataset: str) -> dict | None:\n",
    "    \"\"\"Load metrics JSON file for a specific gauge and dataset.\"\"\"\n",
    "    metrics_file = base_path / gauge_id / f\"{gauge_id}_{dataset}_prediction_metrics.json\"\n",
    "    if metrics_file.exists():\n",
    "        with open(metrics_file) as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_all_metrics(\n",
    "    models: list[str],\n",
    "    datasets: list[str],\n",
    "    gauge_ids: list[str],\n",
    "    rest_path: Path,\n",
    "    ft_path: Path,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load metrics for rest_index and ft_index predictions.\n",
    "\n",
    "    Returns:\n",
    "        rest_metrics: DataFrame with metrics from rest_index predictions\n",
    "        ft_metrics: DataFrame with metrics from ft_index predictions\n",
    "    \"\"\"\n",
    "    poor_data = []\n",
    "    initial_data = []\n",
    "\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            for gauge_id in gauge_ids:\n",
    "                # Load rest_index metrics (from regular predictions folder)\n",
    "                poor_metrics = load_metrics_from_folder(\n",
    "                    rest_path / f\"{model}_poor_gauges\", str(gauge_id), dataset\n",
    "                )\n",
    "                if poor_metrics:\n",
    "                    poor_metrics[\"model\"] = model\n",
    "                    poor_metrics[\"dataset\"] = dataset\n",
    "                    poor_metrics[\"gauge_id\"] = str(gauge_id)\n",
    "                    poor_data.append(poor_metrics)\n",
    "\n",
    "                # Load ft_index metrics (from poor_gauges folder)\n",
    "                initial_metrics = load_metrics_from_folder(\n",
    "                    ft_path / f\"{model}\", str(gauge_id), dataset\n",
    "                )\n",
    "                if initial_metrics:\n",
    "                    initial_metrics[\"model\"] = model\n",
    "                    initial_metrics[\"dataset\"] = dataset\n",
    "                    initial_metrics[\"gauge_id\"] = str(gauge_id)\n",
    "                    initial_data.append(initial_metrics)\n",
    "\n",
    "    poor_df = pd.DataFrame(poor_data)\n",
    "    poor_df.set_index(\"gauge_id\", inplace=True)\n",
    "\n",
    "    initial_df = pd.DataFrame(initial_data)\n",
    "    initial_df.set_index(\"gauge_id\", inplace=True)\n",
    "    return poor_df, initial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "242020c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17-18 period: 0.52\n",
      "19-20 period: -0.08\n"
     ]
    }
   ],
   "source": [
    "model = \"rfr\"\n",
    "meteo = \"mswep\"\n",
    "\n",
    "metrics_17_18 = pd.DataFrame(index=ft_index)\n",
    "metrics_17_18[\"NSE\"] = None\n",
    "metrics_17_18.index.name = \"gauge_id\"\n",
    "metrics_19_20 = pd.DataFrame(index=ft_index)\n",
    "metrics_19_20[\"NSE\"] = None\n",
    "metrics_19_20.index.name = \"gauge_id\"\n",
    "\n",
    "\n",
    "for gauge_id in ft_index:\n",
    "    period_17_18 = pd.read_csv(\n",
    "        f\"../data/predictions/{model}_poor_gauges/{gauge_id}/{gauge_id}_{meteo}_predictions.csv\",\n",
    "        index_col=\"date\",\n",
    "        parse_dates=True,\n",
    "    ).loc[\"2017-01-01\":\"2018-12-31\"]\n",
    "    metrics_17_18.loc[gauge_id, \"NSE\"] = evaluate_model(\n",
    "        observed=period_17_18[\"q_obs\"], simulated=period_17_18[\"q_sim\"]\n",
    "    )[\"NSE\"]\n",
    "\n",
    "\n",
    "for gauge_id in ft_index:\n",
    "    period_19_20 = pd.read_csv(\n",
    "        f\"../data/predictions/{model}/{gauge_id}/{gauge_id}_{meteo}_predictions.csv\",\n",
    "        index_col=\"date\",\n",
    "        parse_dates=True,\n",
    "    ).loc[\"2019-01-01\":\"2020-12-31\"]\n",
    "    metrics_19_20.loc[gauge_id, \"NSE\"] = evaluate_model(\n",
    "        observed=period_19_20[\"q_obs\"], simulated=period_19_20[\"q_sim\"]\n",
    "    )[\"NSE\"]\n",
    "print(f\"17-18 period: {metrics_17_18.median().values[0]:.2f}\")\n",
    "print(f\"19-20 period: {metrics_19_20.median().values[0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43e8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccecf9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b8070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2025-12-26 16:25:29 | INFO     | PhDLogger | fine_tune | ℹ️  Evaluating gpcp...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 265/265 [00:51<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2025-12-26 16:26:21 | INFO     | PhDLogger | fine_tune | ℹ️  Evaluating mswep...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 265/265 [00:52<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2025-12-26 16:27:13 | INFO     | PhDLogger | fine_tune | ℹ️  Evaluating e5l...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 265/265 [00:53<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;39m2025-12-26 16:28:06 | INFO     | PhDLogger | fine_tune | ℹ️  Evaluating e5...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluation: 100%|██████████| 265/265 [00:54<00:00,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "cfg_pathes = {\n",
    "    \"gpcp\": {\n",
    "        \"path\": Path(\n",
    "            \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_gpcp_no_autocorr_static_1203_080402/config.yml\"\n",
    "        ),\n",
    "        \"epoch\": 24,\n",
    "    },\n",
    "    \"mswep\": {\n",
    "        \"path\": Path(\n",
    "            \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_mswep_no_autocorr_static_1103_191754/config.yml\"\n",
    "        ),\n",
    "        \"epoch\": 24,\n",
    "    },\n",
    "    \"e5l\": {\n",
    "        \"path\": Path(\n",
    "            \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_era5l_no_autocorr_static_1003_133332/config.yml\"\n",
    "        ),\n",
    "        \"epoch\": 26,\n",
    "    },\n",
    "    \"e5\": {\n",
    "        \"path\": Path(\n",
    "            \"../data/lstm_configs/model_runs/cudalstm_q_mm_day_era5_no_autocorr_static_1203_220232/config.yml\"\n",
    "        ),\n",
    "        \"epoch\": 20,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"../data/models/fine_tune/poor_gauges.txt\", \"w\") as the_file:\n",
    "    for gauge_name in ft_index:\n",
    "        the_file.write(f\"{int(gauge_name)}\\n\")\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model in [\"gpcp\", \"mswep\", \"e5l\", \"e5\"]:\n",
    "    LOG.info(f\"Evaluating {model}...\")\n",
    "    lstm_cfg = cfg_pathes[model][\"path\"]\n",
    "    epoch = cfg_pathes[model][\"epoch\"]\n",
    "\n",
    "    cfg_run = Config(lstm_cfg)\n",
    "\n",
    "    cfg_run.update_config(\n",
    "        {\n",
    "            \"train_basin_file\": \"../data/models/fine_tune/poor_gauges.txt\",\n",
    "            \"validate_n_random_basins\": len(ft_index),\n",
    "            \"validation_basin_file\": \"../data/models/fine_tune/poor_gauges.txt\",\n",
    "            \"test_basin_file\": \"../data/models/fine_tune/poor_gauges.txt\",\n",
    "            \"test_start_date\": \"01/01/2009\",\n",
    "            \"test_end_date\": \"31/12/2020\",\n",
    "        }\n",
    "    )\n",
    "    tester = get_tester(\n",
    "        cfg=cfg_run, run_dir=cfg_run.run_dir, period=\"test\", init_model=True\n",
    "    )\n",
    "    pred_results = tester.evaluate(epoch=epoch, save_results=True)\n",
    "    model_results[model] = pred_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
