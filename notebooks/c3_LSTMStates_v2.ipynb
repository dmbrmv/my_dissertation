{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro_md",
      "metadata": {},
      "source": [
        "# LSTM States Spatial Analysis (v2)\n",
        "\n",
        "## Scientific Context\n",
        "\n",
        "This notebook analyzes how LSTM hidden states (`h_n`) and cell states (`c_n`) internally represent\n",
        "**meteorological processes NOT included as model inputs**:\n",
        "- Evaporation (GLEAM)\n",
        "- Snow Water Equivalent - SWE (ERA5-Land)\n",
        "- Snow Depth (ERA5-Land)\n",
        "- Subsurface Runoff (ERA5-Land)\n",
        "\n",
        "## LSTM Theory: h_n vs c_n\n",
        "\n",
        "| State | Name | Theoretical Role | Expected Behavior |\n",
        "|-------|------|------------------|-------------------|\n",
        "| **c_n** | Cell State | Long-term memory | Captures **slowly-varying, persistent** patterns (seasonal cycles) |\n",
        "| **h_n** | Hidden State | Short-term output | More **responsive to recent inputs** (quick fluctuations) |\n",
        "\n",
        "**Hypothesis**: \n",
        "- `h_n` should correlate better with quick/variable processes (evaporation, subsurface)\n",
        "- `c_n` should correlate better with seasonal/persistent patterns (SWE, snow_depth)\n",
        "\n",
        "## Data Structure\n",
        "\n",
        "**States array**: `(996 gauges, 730 timesteps, 256 hidden units)`\n",
        "\n",
        "Each gauge has its **own** state trajectory over 730 days (2019-2020 test period).\n",
        "This allows proper per-gauge correlation analysis.\n",
        "\n",
        "## Key Methodological Notes\n",
        "\n",
        "1. **Per-gauge correlation**: Each gauge's state trajectory is correlated with its OWN meteo data\n",
        "2. **Signed correlations**: Preserves positive/negative relationships (cells can inhibit or activate)\n",
        "3. **Z-score ranking**: Ensures all processes get cell representation (prevents snow dominance)\n",
        "4. **Hybrid cluster comparison**: Tests if cell-process mappings align with hydrological regimes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "LSTM States Spatial Analysis (v2 - Per-Gauge Analysis)\n",
        "=======================================================\n",
        "Analyze how LSTM hidden states (h_n) and cell states (c_n) internally represent\n",
        "meteorological processes NOT included as model inputs.\n",
        "\n",
        "Key Features:\n",
        "- Per-gauge state trajectories: Each gauge's states correlated with its own meteo\n",
        "- Preserves correlation SIGN (positive vs negative groups)\n",
        "- Analyzes BOTH h_n and c_n with comparison\n",
        "- Z-score ranking for balanced process representation\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import cartopy.crs as ccrs\n",
        "import geopandas as gpd\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "from src.plots.hex_maps import hexes_plots_n\n",
        "from src.readers.geom_reader import load_geodata\n",
        "from src.utils.logger import setup_logger\n",
        "\n",
        "plt.rcParams[\"font.family\"] = \"DeJavu Serif\"\n",
        "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
        "log = setup_logger(\"chapter_three_v2\", log_file=\"../logs/chapter_three_v2.log\")\n",
        "\n",
        "# Output directories\n",
        "table_dir = Path(\"../res/chapter_three/tables\")\n",
        "table_dir.mkdir(parents=True, exist_ok=True)\n",
        "image_dir = Path(\"../res/chapter_three/images\")\n",
        "image_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Imports complete. Output directories ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "data_loading_md",
      "metadata": {},
      "source": [
        "## 1. Data Loading\n",
        "\n",
        "Load:\n",
        "- **LSTM states**: `(996, 730, 256)` - per-gauge state trajectories\n",
        "- **Gauge geometries**: For spatial visualization\n",
        "- **Hybrid clusters**: Pre-computed hydrological regime classifications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load_data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Load Data: LSTM states, gauge geometries, hybrid clusters\n",
        "# =============================================================================\n",
        "\n",
        "# Load gauge geometries\n",
        "ws, gauges = load_geodata(folder_depth=\"../\")\n",
        "basemap_data = gpd.read_file(\"../data/geometry/basemap_2023.gpkg\")\n",
        "\n",
        "# Load hybrid cluster assignments\n",
        "gauge_mapping = pd.read_csv(\n",
        "    \"../res/chapter_one/gauge_hybrid_mapping.csv\",\n",
        "    index_col=\"gauge_id\",\n",
        "    dtype={\"gauge_id\": str},\n",
        ")\n",
        "hybrid_clusters = gauge_mapping[\"hybrid_class\"]\n",
        "\n",
        "# Load LSTM states - NEW FORMAT: (n_gauges, n_timesteps, hidden_size)\n",
        "states_path = Path(\"../data/optimization/lstm_states_per_gauge/all_gauges_states.npz\")\n",
        "print(f\"Loading LSTM states from: {states_path}\")\n",
        "states = np.load(states_path, allow_pickle=True)\n",
        "print(f\"Available keys: {list(states.keys())}\")\n",
        "\n",
        "# Extract state arrays\n",
        "# Shape: (n_gauges, n_timesteps, hidden_size) = (996, 730, 256)\n",
        "h_states_all = states[\"h_n\"]  # Hidden states for all gauges\n",
        "c_states_all = states[\"c_n\"]  # Cell states for all gauges\n",
        "\n",
        "# Get gauge IDs from states file (ordering matches array indices)\n",
        "if \"gauge_ids\" in states.keys():\n",
        "    state_gauge_ids = [str(g) for g in states[\"gauge_ids\"]]\n",
        "else:\n",
        "    # Fallback: use common_index order\n",
        "    state_gauge_ids = gauges.index.to_list()\n",
        "\n",
        "n_gauges, n_timesteps, hidden_size = h_states_all.shape\n",
        "\n",
        "print(f\"\\nh_n shape: {h_states_all.shape}\")\n",
        "print(f\"c_n shape: {c_states_all.shape}\")\n",
        "print(f\"Gauges in states: {n_gauges}\")\n",
        "print(f\"Timesteps per gauge: {n_timesteps}\")\n",
        "print(f\"Hidden size: {hidden_size}\")\n",
        "print(f\"Hybrid clusters: {hybrid_clusters.nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "meteo_params_md",
      "metadata": {},
      "source": [
        "## 2. Meteorological Parameters\n",
        "\n",
        "These are processes the LSTM **never saw during training**, but may have learned to represent internally:\n",
        "\n",
        "| Parameter | Source | Physical Meaning |\n",
        "|-----------|--------|------------------|\n",
        "| Evaporation | GLEAM | Water loss to atmosphere |\n",
        "| SWE | ERA5-Land | Snow water storage |\n",
        "| Snow Depth | ERA5-Land | Snow accumulation |\n",
        "| Subsurface | ERA5-Land | Groundwater/baseflow contribution |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "meteo_params",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Define meteorological parameters (NOT used as model inputs)\n",
        "# =============================================================================\n",
        "METEO_PARAMS = {\n",
        "    \"evaporation\": {\n",
        "        \"path\": Path(\"../data/meteo_grids_2024/gleam/E\"),\n",
        "        \"column\": \"E\",\n",
        "        \"description\": \"Evaporation (GLEAM)\",\n",
        "        \"temporal_scale\": \"short-term\",  # Quick/variable process\n",
        "    },\n",
        "    \"swe\": {\n",
        "        \"path\": Path(\n",
        "            \"../data/meteo_grids_2024/snow_and_subsurface/era5_land/snow_depth_water_equivalent\"\n",
        "        ),\n",
        "        \"column\": \"swe_e5l\",\n",
        "        \"description\": \"Snow Water Equivalent (ERA5-Land)\",\n",
        "        \"temporal_scale\": \"seasonal\",  # Slow/persistent process\n",
        "    },\n",
        "    \"snow_depth\": {\n",
        "        \"path\": Path(\"../data/meteo_grids_2024/snow_and_subsurface/era5_land/snow_depth\"),\n",
        "        \"column\": None,\n",
        "        \"description\": \"Snow Depth (ERA5-Land)\",\n",
        "        \"temporal_scale\": \"seasonal\",  # Slow/persistent process\n",
        "    },\n",
        "    \"subsurface\": {\n",
        "        \"path\": Path(\n",
        "            \"../data/meteo_grids_2024/snow_and_subsurface/era5_land/sub_surface_runoff\"\n",
        "        ),\n",
        "        \"column\": None,\n",
        "        \"description\": \"Subsurface Runoff (ERA5-Land)\",\n",
        "        \"temporal_scale\": \"short-term\",  # Quick/variable process\n",
        "    },\n",
        "}\n",
        "\n",
        "# Color scheme for signed process groups\n",
        "PROCESS_COLORS = {\n",
        "    \"evaporation_pos\": \"#27ae60\",  # Green\n",
        "    \"evaporation_neg\": \"#1e8449\",  # Dark Green\n",
        "    \"swe_pos\": \"#3498db\",  # Blue\n",
        "    \"swe_neg\": \"#1a5276\",  # Dark Blue\n",
        "    \"snow_depth_pos\": \"#9b59b6\",  # Purple\n",
        "    \"snow_depth_neg\": \"#6c3483\",  # Dark Purple\n",
        "    \"subsurface_pos\": \"#e67e22\",  # Orange\n",
        "    \"subsurface_neg\": \"#a04000\",  # Dark Orange\n",
        "    \"inactive\": \"#bdc3c7\",  # Gray\n",
        "}\n",
        "\n",
        "# Minimum correlation threshold\n",
        "MIN_CORRELATION = 0.3\n",
        "\n",
        "# Verify paths\n",
        "for param, info in METEO_PARAMS.items():\n",
        "    exists = info[\"path\"].exists()\n",
        "    n_files = len(list(info[\"path\"].glob(\"*.csv\"))) if exists else 0\n",
        "    print(f\"{param}: exists={exists}, files={n_files}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "correlation_md",
      "metadata": {},
      "source": [
        "## 3. Per-Gauge Correlation Analysis\n",
        "\n",
        "**Critical improvement over previous version**:\n",
        "\n",
        "- **Before**: One state trajectory correlated with 996 different gauges' meteo data\n",
        "- **Now**: Each gauge's state trajectory correlated with its OWN meteo data\n",
        "\n",
        "For each gauge:\n",
        "1. Get gauge's state trajectory: `(730 timesteps, 256 cells)`\n",
        "2. Load gauge's meteo data: `(730 timesteps,)`\n",
        "3. Compute correlation for each cell: `r = corr(cell_trajectory, meteo_trajectory)`\n",
        "\n",
        "Result: Per-gauge correlation matrix `(n_gauges, n_cells)` for each meteo parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "correlation_functions",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Core correlation functions (preserving sign)\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def compute_cell_correlations(\n",
        "    cell_states: np.ndarray,  # Shape: (n_timesteps, hidden_size)\n",
        "    meteo_array: np.ndarray,  # Shape: (n_timesteps,)\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute SIGNED Pearson correlation between each LSTM cell and meteo data.\n",
        "\n",
        "    Args:\n",
        "        cell_states: LSTM states for one gauge, shape (n_timesteps, hidden_size)\n",
        "        meteo_array: Meteo values for same gauge, shape (n_timesteps,)\n",
        "\n",
        "    Returns:\n",
        "        Array of shape (hidden_size,) with signed correlation coefficients.\n",
        "    \"\"\"\n",
        "    # Align lengths\n",
        "    min_len = min(len(cell_states), len(meteo_array))\n",
        "    cell_states = cell_states[:min_len]\n",
        "    meteo_array = meteo_array[:min_len]\n",
        "\n",
        "    # Remove NaN values\n",
        "    valid_mask = ~np.isnan(meteo_array)\n",
        "    if valid_mask.sum() < 30:  # Minimum samples for meaningful correlation\n",
        "        return np.full(cell_states.shape[1], np.nan)\n",
        "\n",
        "    cell_states = cell_states[valid_mask]\n",
        "    meteo_array = meteo_array[valid_mask]\n",
        "\n",
        "    # Compute correlation for each cell\n",
        "    n_cells = cell_states.shape[1]\n",
        "    correlations = np.zeros(n_cells)\n",
        "\n",
        "    for i in range(n_cells):\n",
        "        try:\n",
        "            corr_matrix = np.corrcoef(cell_states[:, i], meteo_array)\n",
        "            correlations[i] = corr_matrix[0, 1]  # SIGNED correlation\n",
        "        except Exception:\n",
        "            correlations[i] = np.nan\n",
        "\n",
        "    return correlations\n",
        "\n",
        "\n",
        "def load_meteo_data(\n",
        "    gauge_id: str,\n",
        "    param_name: str,\n",
        "    start_date: str = \"2019-01-01\",\n",
        "    end_date: str = \"2020-12-31\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Load meteorological data for a specific gauge.\n",
        "    Filters to test period dates to match LSTM states.\n",
        "    \"\"\"\n",
        "    info = METEO_PARAMS[param_name]\n",
        "    file_path = info[\"path\"] / f\"{gauge_id}.csv\"\n",
        "\n",
        "    if not file_path.exists():\n",
        "        return np.array([])\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, index_col=\"date\", parse_dates=[\"date\"])\n",
        "\n",
        "        # Filter to test period\n",
        "        df = df.loc[start_date:end_date]\n",
        "\n",
        "        col = info[\"column\"] if info[\"column\"] else df.columns[0]\n",
        "        if col not in df.columns:\n",
        "            col = df.columns[0]\n",
        "        return df[col].values\n",
        "    except Exception as e:\n",
        "        return np.array([])\n",
        "\n",
        "\n",
        "print(\"Correlation functions defined (sign-preserving).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compute_correlations",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Compute per-gauge correlations for h_n and c_n\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def compute_all_gauge_correlations(states_array, state_name=\"h_n\"):\n",
        "    \"\"\"\n",
        "    Compute correlation matrices for all gauges and all meteo params.\n",
        "\n",
        "    Args:\n",
        "        states_array: Shape (n_gauges, n_timesteps, hidden_size)\n",
        "        state_name: For logging\n",
        "\n",
        "    Returns:\n",
        "        Dict[param_name -> DataFrame(gauge_id x cell_id)]\n",
        "    \"\"\"\n",
        "    correlation_matrices = {}\n",
        "\n",
        "    for param_name in METEO_PARAMS.keys():\n",
        "        print(f\"\\n{state_name} - Processing: {param_name}\")\n",
        "\n",
        "        # Initialize correlation DataFrame\n",
        "        corr_df = pd.DataFrame(\n",
        "            index=state_gauge_ids,\n",
        "            columns=range(hidden_size),\n",
        "            dtype=float,\n",
        "        )\n",
        "\n",
        "        success_count = 0\n",
        "        for gauge_idx, gauge_id in enumerate(tqdm(state_gauge_ids, desc=param_name)):\n",
        "            # Get THIS gauge's state trajectory\n",
        "            gauge_states = states_array[gauge_idx]  # Shape: (n_timesteps, hidden_size)\n",
        "\n",
        "            # Load THIS gauge's meteo data\n",
        "            meteo_data = load_meteo_data(gauge_id, param_name)\n",
        "\n",
        "            if len(meteo_data) == 0:\n",
        "                continue\n",
        "\n",
        "            # Compute correlations between gauge's states and gauge's meteo\n",
        "            correlations = compute_cell_correlations(gauge_states, meteo_data)\n",
        "\n",
        "            if not np.all(np.isnan(correlations)):\n",
        "                corr_df.loc[gauge_id] = correlations\n",
        "                success_count += 1\n",
        "\n",
        "        correlation_matrices[param_name] = corr_df.dropna(how=\"all\")\n",
        "        print(\n",
        "            f\"  Valid gauges: {success_count}, Shape: {correlation_matrices[param_name].shape}\"\n",
        "        )\n",
        "\n",
        "    return correlation_matrices\n",
        "\n",
        "\n",
        "# Compute for h_n (hidden state)\n",
        "print(\"=\" * 70)\n",
        "print(\"Computing SIGNED correlations for h_n (hidden state)\")\n",
        "print(\"Each gauge's states correlated with its OWN meteo data\")\n",
        "print(\"=\" * 70)\n",
        "h_correlation_matrices = compute_all_gauge_correlations(h_states_all, \"h_n\")\n",
        "\n",
        "# Compute for c_n (cell state)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Computing SIGNED correlations for c_n (cell state)\")\n",
        "print(\"Each gauge's states correlated with its OWN meteo data\")\n",
        "print(\"=\" * 70)\n",
        "c_correlation_matrices = compute_all_gauge_correlations(c_states_all, \"c_n\")\n",
        "\n",
        "print(\"\\nCorrelation computation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_assignment_md",
      "metadata": {},
      "source": [
        "## 4. Cell Assignment to Process Groups\n",
        "\n",
        "Assign each of the 256 LSTM cells to a \"signed process group\" based on which\n",
        "meteorological process it best represents.\n",
        "\n",
        "**Z-score ranking approach**:\n",
        "- Problem: Snow processes often have higher raw correlations, dominating assignments\n",
        "- Solution: Normalize correlations within each process using Z-scores\n",
        "- This ensures weaker-correlation processes (subsurface, evaporation) still get cells\n",
        "\n",
        "Groups: `{process}_pos` (positive correlation) or `{process}_neg` (negative correlation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_assignment",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Assign cells to SIGNED process groups using Z-SCORE ranking\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def assign_cells_to_signed_groups(correlation_matrices, state_name=\"h_n\"):\n",
        "    \"\"\"\n",
        "    Assign each cell to a signed process group using Z-score ranking.\n",
        "\n",
        "    This method normalizes correlations within each process so that\n",
        "    processes with inherently weaker correlations still get cells assigned.\n",
        "    \"\"\"\n",
        "    all_params = list(METEO_PARAMS.keys())\n",
        "\n",
        "    # Compute mean correlation per cell across all gauges\n",
        "    cell_corr_matrix = pd.DataFrame(index=range(hidden_size))\n",
        "\n",
        "    for param_name in all_params:\n",
        "        corr_df = correlation_matrices[param_name]\n",
        "        mean_corr = corr_df.mean()  # Mean across gauges, preserves sign!\n",
        "        cell_corr_matrix[param_name] = mean_corr.values\n",
        "\n",
        "    # Compute Z-scores for each cell within each process\n",
        "    # This normalizes so we compare \"how good is this cell for this process\"\n",
        "    z_scores = pd.DataFrame(index=range(hidden_size))\n",
        "\n",
        "    for param in all_params:\n",
        "        col = cell_corr_matrix[param]\n",
        "        # Z-score for positive correlations\n",
        "        z_scores[f\"{param}_pos\"] = (col.clip(lower=0) - col.clip(lower=0).mean()) / (\n",
        "            col.clip(lower=0).std() + 1e-8\n",
        "        )\n",
        "        # Z-score for negative correlations (use absolute value)\n",
        "        z_scores[f\"{param}_neg\"] = (\n",
        "            col.clip(upper=0).abs() - col.clip(upper=0).abs().mean()\n",
        "        ) / (col.clip(upper=0).abs().std() + 1e-8)\n",
        "\n",
        "    # For each cell, find which signed group has highest Z-score\n",
        "    cell_assignment = []\n",
        "\n",
        "    for cell_id in range(hidden_size):\n",
        "        cell_zscores = z_scores.loc[cell_id]\n",
        "        cell_corrs = cell_corr_matrix.loc[cell_id]\n",
        "\n",
        "        # Find best group by Z-score\n",
        "        best_group = cell_zscores.idxmax()\n",
        "        best_zscore = cell_zscores.max()\n",
        "\n",
        "        # Get the actual correlation for this group\n",
        "        base_process = best_group.rsplit(\"_\", 1)[0]\n",
        "        is_positive = best_group.endswith(\"_pos\")\n",
        "\n",
        "        if is_positive:\n",
        "            actual_r = cell_corrs[base_process]\n",
        "        else:\n",
        "            actual_r = -abs(cell_corrs[base_process])\n",
        "\n",
        "        # Check if correlation meets minimum threshold\n",
        "        if abs(actual_r) >= MIN_CORRELATION:\n",
        "            primary = best_group\n",
        "            primary_r = actual_r\n",
        "        else:\n",
        "            # Fall back to absolute best if Z-score winner doesn't meet threshold\n",
        "            all_corrs = cell_corrs.abs()\n",
        "            if all_corrs.max() >= MIN_CORRELATION:\n",
        "                best_param = all_corrs.idxmax()\n",
        "                best_r = cell_corrs[best_param]\n",
        "                primary = f\"{best_param}_pos\" if best_r > 0 else f\"{best_param}_neg\"\n",
        "                primary_r = best_r\n",
        "            else:\n",
        "                primary = \"inactive\"\n",
        "                primary_r = 0\n",
        "\n",
        "        cell_assignment.append(\n",
        "            {\n",
        "                \"cell_id\": cell_id,\n",
        "                \"primary_group\": primary,\n",
        "                \"primary_r\": primary_r,\n",
        "                \"zscore\": best_zscore,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    cell_assignment_df = pd.DataFrame(cell_assignment)\n",
        "\n",
        "    # Create cell groups dictionary\n",
        "    cell_groups = {}\n",
        "    for group in cell_assignment_df[\"primary_group\"].unique():\n",
        "        cell_groups[group] = cell_assignment_df[\n",
        "            cell_assignment_df[\"primary_group\"] == group\n",
        "        ][\"cell_id\"].tolist()\n",
        "\n",
        "    return cell_assignment_df, cell_groups, cell_corr_matrix\n",
        "\n",
        "\n",
        "# Apply to h_n\n",
        "print(\"=\" * 70)\n",
        "print(\"Assigning h_n cells to SIGNED process groups (Z-score ranking)\")\n",
        "print(\"=\" * 70)\n",
        "h_cell_assignment, h_cell_groups, h_cell_corr_matrix = assign_cells_to_signed_groups(\n",
        "    h_correlation_matrices, \"h_n\"\n",
        ")\n",
        "\n",
        "# Print mean |r| per process\n",
        "print(\"\\nMean |r| per process across all cells:\")\n",
        "for param in METEO_PARAMS.keys():\n",
        "    mean_abs_r = h_cell_corr_matrix[param].abs().mean()\n",
        "    print(f\"  {param}: {mean_abs_r:.3f}\")\n",
        "\n",
        "print(\"\\nh_n Cell Group Distribution:\")\n",
        "print(h_cell_assignment[\"primary_group\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "visualization_md",
      "metadata": {},
      "source": [
        "## 5. Cell Grid Visualization\n",
        "\n",
        "Visualize the 256 LSTM cells as a 16x16 grid, colored by their assigned process group.\n",
        "\n",
        "This shows how the LSTM internally organized its representations across cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_grid",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 16x16 Grid Visualization of Cell Assignments\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def plot_cell_grid(cell_assignment_df, title_suffix=\"\"):\n",
        "    \"\"\"Plot 16x16 grid of cells colored by process group.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(14, 12))\n",
        "\n",
        "    grid_size = int(np.sqrt(hidden_size))\n",
        "    cell_grid = np.zeros((grid_size, grid_size, 3))\n",
        "\n",
        "    # Convert hex colors to RGB\n",
        "    def hex_to_rgb(hex_color):\n",
        "        hex_color = hex_color.lstrip(\"#\")\n",
        "        return tuple(int(hex_color[i : i + 2], 16) / 255 for i in (0, 2, 4))\n",
        "\n",
        "    for _, row in cell_assignment_df.iterrows():\n",
        "        cell_id = int(row[\"cell_id\"])\n",
        "        group = row[\"primary_group\"]\n",
        "        color = PROCESS_COLORS.get(group, PROCESS_COLORS[\"inactive\"])\n",
        "\n",
        "        i, j = cell_id // grid_size, cell_id % grid_size\n",
        "        cell_grid[i, j] = hex_to_rgb(color)\n",
        "\n",
        "    ax.imshow(cell_grid, aspect=\"equal\")\n",
        "\n",
        "    # Grid lines\n",
        "    for i in range(grid_size + 1):\n",
        "        ax.axhline(i - 0.5, color=\"white\", linewidth=0.5)\n",
        "        ax.axvline(i - 0.5, color=\"white\", linewidth=0.5)\n",
        "\n",
        "    ax.set_xticks(range(0, grid_size, 2))\n",
        "    ax.set_yticks(range(0, grid_size, 2))\n",
        "    ax.set_xlabel(\"Cell Column\", fontsize=12)\n",
        "    ax.set_ylabel(\"Cell Row\", fontsize=12)\n",
        "    ax.set_title(f\"LSTM Cell Process Groups{title_suffix}\", fontsize=14)\n",
        "\n",
        "    # Legend\n",
        "    legend_elements = [\n",
        "        Patch(facecolor=color, label=group.replace(\"_\", \" \").title())\n",
        "        for group, color in PROCESS_COLORS.items()\n",
        "        if group in cell_assignment_df[\"primary_group\"].values\n",
        "    ]\n",
        "    ax.legend(\n",
        "        handles=legend_elements,\n",
        "        loc=\"center left\",\n",
        "        bbox_to_anchor=(1.02, 0.5),\n",
        "        fontsize=10,\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# Plot h_n cell grid\n",
        "fig = plot_cell_grid(h_cell_assignment, \" (h_n - Hidden State)\")\n",
        "fig.savefig(image_dir / \"lstm_v2_hn_cell_grid.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gauge_dominant_md",
      "metadata": {},
      "source": [
        "## 6. Per-Gauge Dominant Process\n",
        "\n",
        "For each gauge, determine which process has the strongest representation.\n",
        "\n",
        "**Method**: Use Z-score normalized correlations to find which process dominates\n",
        "for each gauge, ensuring fair comparison across processes with different\n",
        "inherent correlation magnitudes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gauge_dominant",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Per-Gauge Dominant Process (with Z-score normalization)\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def compute_gauge_dominant_process(correlation_matrices):\n",
        "    \"\"\"\n",
        "    For each gauge, find which signed process has the strongest representation.\n",
        "    Uses Z-score normalization for fair comparison across processes.\n",
        "    \"\"\"\n",
        "    # Get common gauges across all params\n",
        "    common_gauges = set(state_gauge_ids)\n",
        "    for param in correlation_matrices.values():\n",
        "        common_gauges &= set(param.index)\n",
        "    common_gauges = list(common_gauges)\n",
        "\n",
        "    # Compute max absolute correlation per gauge per process\n",
        "    gauge_max_corr = pd.DataFrame(index=common_gauges)\n",
        "\n",
        "    for param_name, corr_df in correlation_matrices.items():\n",
        "        # For each gauge, get max absolute correlation across all cells\n",
        "        gauge_max_corr[f\"{param_name}_max\"] = corr_df.loc[common_gauges].abs().max(axis=1)\n",
        "        # Also track the sign of the best cell\n",
        "        best_cell_idx = corr_df.loc[common_gauges].abs().idxmax(axis=1)\n",
        "        gauge_max_corr[f\"{param_name}_sign\"] = [\n",
        "            np.sign(corr_df.loc[g, idx]) for g, idx in zip(common_gauges, best_cell_idx)\n",
        "        ]\n",
        "\n",
        "    # Z-score normalize within each process\n",
        "    gauge_zscores = pd.DataFrame(index=common_gauges)\n",
        "    for param_name in METEO_PARAMS.keys():\n",
        "        col = gauge_max_corr[f\"{param_name}_max\"]\n",
        "        gauge_zscores[param_name] = (col - col.mean()) / (col.std() + 1e-8)\n",
        "\n",
        "    # Find dominant process for each gauge\n",
        "    gauge_dominant = []\n",
        "\n",
        "    for gauge_id in common_gauges:\n",
        "        # Best process by Z-score\n",
        "        best_param = gauge_zscores.loc[gauge_id].idxmax()\n",
        "        best_r = gauge_max_corr.loc[gauge_id, f\"{best_param}_max\"]\n",
        "        best_sign = gauge_max_corr.loc[gauge_id, f\"{best_param}_sign\"]\n",
        "\n",
        "        if best_r >= MIN_CORRELATION:\n",
        "            group = f\"{best_param}_pos\" if best_sign > 0 else f\"{best_param}_neg\"\n",
        "        else:\n",
        "            group = \"inactive\"\n",
        "            best_r = 0\n",
        "\n",
        "        gauge_dominant.append(\n",
        "            {\n",
        "                \"gauge_id\": gauge_id,\n",
        "                \"dominant_group\": group,\n",
        "                \"max_r\": best_r * best_sign,\n",
        "                \"zscore\": gauge_zscores.loc[gauge_id, best_param]\n",
        "                if group != \"inactive\"\n",
        "                else 0,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(gauge_dominant).set_index(\"gauge_id\")\n",
        "\n",
        "\n",
        "# Compute for h_n\n",
        "h_gauge_dominant = compute_gauge_dominant_process(h_correlation_matrices)\n",
        "\n",
        "print(\"h_n Gauge Dominant Process Distribution:\")\n",
        "print(h_gauge_dominant[\"dominant_group\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spatial_md",
      "metadata": {},
      "source": [
        "## 7. Spatial Visualization\n",
        "\n",
        "Map the dominant process for each gauge to see spatial patterns.\n",
        "\n",
        "Expected patterns:\n",
        "- Snow processes should dominate in northern/mountain regions\n",
        "- Evaporation should dominate in southern/agricultural regions\n",
        "- Subsurface should appear in regions with significant groundwater contribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spatial_map",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Spatial Map of Dominant Processes\n",
        "# =============================================================================\n",
        "\n",
        "# Merge with gauge geometries\n",
        "gauges_with_process = gauges.copy()\n",
        "gauges_with_process = gauges_with_process.join(h_gauge_dominant, how=\"inner\")\n",
        "\n",
        "# Create map\n",
        "fig, ax = plt.subplots(\n",
        "    figsize=(14, 10),\n",
        "    subplot_kw={\n",
        "        \"projection\": ccrs.AlbersEqualArea(central_longitude=100, central_latitude=60)\n",
        "    },\n",
        ")\n",
        "\n",
        "# Plot basemap\n",
        "basemap_data.to_crs(ax.projection.proj4_init).plot(\n",
        "    ax=ax, color=\"lightgray\", edgecolor=\"white\", linewidth=0.5\n",
        ")\n",
        "\n",
        "# Plot gauges by dominant process\n",
        "for group in gauges_with_process[\"dominant_group\"].unique():\n",
        "    subset = gauges_with_process[gauges_with_process[\"dominant_group\"] == group]\n",
        "    color = PROCESS_COLORS.get(group, PROCESS_COLORS[\"inactive\"])\n",
        "    subset.to_crs(ax.projection.proj4_init).plot(\n",
        "        ax=ax,\n",
        "        color=color,\n",
        "        markersize=15,\n",
        "        alpha=0.8,\n",
        "        label=group.replace(\"_\", \" \").title(),\n",
        "    )\n",
        "\n",
        "ax.set_title(\n",
        "    \"Dominant LSTM Process (h_n) by Gauge\\n(Z-score normalized for fair comparison)\",\n",
        "    fontsize=14,\n",
        ")\n",
        "ax.legend(loc=\"lower left\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(\n",
        "    image_dir / \"lstm_v2_hn_dominant_process_map.png\", dpi=150, bbox_inches=\"tight\"\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Watersheds with valid data: {len(gauges_with_process)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cn_comparison_md",
      "metadata": {},
      "source": [
        "## 8. h_n vs c_n Comparison\n",
        "\n",
        "Compare hidden state (h_n) and cell state (c_n) to test the hypothesis:\n",
        "\n",
        "- **h_n**: Should specialize in quick/variable processes (evaporation, subsurface)\n",
        "- **c_n**: Should specialize in seasonal/persistent patterns (SWE, snow_depth)\n",
        "\n",
        "We compare:\n",
        "1. Cell group distributions\n",
        "2. Overlap between h_n and c_n assignments\n",
        "3. Mean correlation strengths per process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cn_assignment",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Apply same analysis to c_n (cell state)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Assigning c_n cells to SIGNED process groups\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "c_cell_assignment, c_cell_groups, c_cell_corr_matrix = assign_cells_to_signed_groups(\n",
        "    c_correlation_matrices, \"c_n\"\n",
        ")\n",
        "\n",
        "print(\"\\nc_n Cell Group Distribution:\")\n",
        "print(c_cell_assignment[\"primary_group\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hn_cn_comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Side-by-side Grid Comparison: h_n vs c_n\n",
        "# =============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n",
        "\n",
        "grid_size = int(np.sqrt(hidden_size))\n",
        "\n",
        "\n",
        "def hex_to_rgb(hex_color):\n",
        "    hex_color = hex_color.lstrip(\"#\")\n",
        "    return tuple(int(hex_color[i : i + 2], 16) / 255 for i in (0, 2, 4))\n",
        "\n",
        "\n",
        "for ax_idx, (cell_assign, title) in enumerate(\n",
        "    [\n",
        "        (h_cell_assignment, \"Hidden State (h_n)\\nShort-term memory\"),\n",
        "        (c_cell_assignment, \"Cell State (c_n)\\nLong-term memory\"),\n",
        "    ]\n",
        "):\n",
        "    ax = axes[ax_idx]\n",
        "    cell_grid = np.zeros((grid_size, grid_size, 3))\n",
        "\n",
        "    for _, row in cell_assign.iterrows():\n",
        "        cell_id = int(row[\"cell_id\"])\n",
        "        group = row[\"primary_group\"]\n",
        "        color = PROCESS_COLORS.get(group, PROCESS_COLORS[\"inactive\"])\n",
        "        i, j = cell_id // grid_size, cell_id % grid_size\n",
        "        cell_grid[i, j] = hex_to_rgb(color)\n",
        "\n",
        "    ax.imshow(cell_grid, aspect=\"equal\")\n",
        "\n",
        "    for i in range(grid_size + 1):\n",
        "        ax.axhline(i - 0.5, color=\"white\", linewidth=0.5)\n",
        "        ax.axvline(i - 0.5, color=\"white\", linewidth=0.5)\n",
        "\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    ax.set_xlabel(\"Cell Column\")\n",
        "    ax.set_ylabel(\"Cell Row\")\n",
        "\n",
        "# Shared legend\n",
        "legend_elements = [\n",
        "    Patch(facecolor=color, label=group.replace(\"_\", \" \").title())\n",
        "    for group, color in PROCESS_COLORS.items()\n",
        "]\n",
        "fig.legend(\n",
        "    handles=legend_elements, loc=\"center right\", bbox_to_anchor=(1.08, 0.5), fontsize=10\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(image_dir / \"lstm_v2_hn_cn_comparison_grid.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "comparison_stats",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Comparison Statistics: h_n vs c_n\n",
        "# =============================================================================\n",
        "\n",
        "comparison_data = []\n",
        "\n",
        "all_groups = set(h_cell_groups.keys()) | set(c_cell_groups.keys())\n",
        "\n",
        "for group in sorted(all_groups):\n",
        "    h_cells = set(h_cell_groups.get(group, []))\n",
        "    c_cells = set(c_cell_groups.get(group, []))\n",
        "\n",
        "    overlap = h_cells & c_cells\n",
        "    h_only = h_cells - c_cells\n",
        "    c_only = c_cells - h_cells\n",
        "\n",
        "    comparison_data.append(\n",
        "        {\n",
        "            \"Group\": group,\n",
        "            \"h_n\": len(h_cells),\n",
        "            \"c_n\": len(c_cells),\n",
        "            \"Overlap\": len(overlap),\n",
        "            \"h_n only\": len(h_only),\n",
        "            \"c_n only\": len(c_only),\n",
        "        }\n",
        "    )\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"=\" * 70)\n",
        "print(\"h_n vs c_n COMPARISON\")\n",
        "print(\"=\" * 70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Overall agreement\n",
        "same_assignment = sum(\n",
        "    1\n",
        "    for i in range(hidden_size)\n",
        "    if h_cell_assignment.loc[i, \"primary_group\"]\n",
        "    == c_cell_assignment.loc[i, \"primary_group\"]\n",
        ")\n",
        "print(\n",
        "    f\"\\nCells with same assignment: {same_assignment}/{hidden_size} ({100 * same_assignment / hidden_size:.1f}%)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hypothesis_test_md",
      "metadata": {},
      "source": [
        "## 9. Hypothesis Test: Temporal Scale Specialization\n",
        "\n",
        "Test whether h_n and c_n show different preferences for temporal scales:\n",
        "\n",
        "| Process | Expected Temporal Scale | Expected State |\n",
        "|---------|------------------------|----------------|\n",
        "| Evaporation | Short-term (daily fluctuations) | h_n |\n",
        "| Subsurface | Short-term (event-driven) | h_n |\n",
        "| SWE | Seasonal (slow accumulation/melt) | c_n |\n",
        "| Snow Depth | Seasonal (winter pattern) | c_n |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hypothesis_test",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Hypothesis Test: Which state better captures which process?\n",
        "# =============================================================================\n",
        "\n",
        "# Compute mean absolute correlation per process for h_n and c_n\n",
        "process_comparison = []\n",
        "\n",
        "for param_name, info in METEO_PARAMS.items():\n",
        "    h_mean_r = h_cell_corr_matrix[param_name].abs().mean()\n",
        "    c_mean_r = c_cell_corr_matrix[param_name].abs().mean()\n",
        "\n",
        "    # Which state has stronger correlation?\n",
        "    better_state = \"h_n\" if h_mean_r > c_mean_r else \"c_n\"\n",
        "    expected_state = \"h_n\" if info[\"temporal_scale\"] == \"short-term\" else \"c_n\"\n",
        "    matches_hypothesis = better_state == expected_state\n",
        "\n",
        "    process_comparison.append(\n",
        "        {\n",
        "            \"Process\": param_name,\n",
        "            \"Temporal Scale\": info[\"temporal_scale\"],\n",
        "            \"h_n Mean |r|\": h_mean_r,\n",
        "            \"c_n Mean |r|\": c_mean_r,\n",
        "            \"Better State\": better_state,\n",
        "            \"Expected\": expected_state,\n",
        "            \"Matches Hypothesis\": matches_hypothesis,\n",
        "        }\n",
        "    )\n",
        "\n",
        "hypothesis_df = pd.DataFrame(process_comparison)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"HYPOTHESIS TEST: h_n (short-term) vs c_n (long-term)\")\n",
        "print(\"=\" * 70)\n",
        "print(hypothesis_df.to_string(index=False))\n",
        "\n",
        "matches = hypothesis_df[\"Matches Hypothesis\"].sum()\n",
        "total = len(hypothesis_df)\n",
        "print(f\"\\nHypothesis matches: {matches}/{total} processes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cluster_md",
      "metadata": {},
      "source": [
        "## 10. Cluster Generalization Analysis\n",
        "\n",
        "Compare LSTM cell-process mappings with hybrid clusters to test:\n",
        "- Do gauges in the same hybrid cluster have similar dominant processes?\n",
        "- Does the LSTM's internal organization align with hydrological regimes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cluster_analysis",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Cluster vs Dominant Process Cross-tabulation\n",
        "# =============================================================================\n",
        "\n",
        "# Merge hybrid clusters with gauge dominant process\n",
        "cluster_process = h_gauge_dominant.copy()\n",
        "cluster_process[\"hybrid_cluster\"] = hybrid_clusters.reindex(cluster_process.index)\n",
        "cluster_process = cluster_process.dropna(subset=[\"hybrid_cluster\"])\n",
        "\n",
        "# Cross-tabulation\n",
        "crosstab = pd.crosstab(\n",
        "    cluster_process[\"hybrid_cluster\"],\n",
        "    cluster_process[\"dominant_group\"],\n",
        "    margins=True,\n",
        ")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"HYBRID CLUSTER vs DOMINANT PROCESS CROSS-TABULATION\")\n",
        "print(\"=\" * 70)\n",
        "print(crosstab.to_string())\n",
        "\n",
        "# Save\n",
        "crosstab.to_csv(table_dir / \"lstm_v2_cluster_process_crosstab.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cluster_heatmap",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Heatmap of Cluster-Process Proportions\n",
        "# =============================================================================\n",
        "\n",
        "# Compute proportions within each cluster\n",
        "crosstab_norm = crosstab.drop(\"All\", axis=0).drop(\"All\", axis=1)\n",
        "crosstab_pct = crosstab_norm.div(crosstab_norm.sum(axis=1), axis=0) * 100\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "sns.heatmap(\n",
        "    crosstab_pct,\n",
        "    annot=True,\n",
        "    fmt=\".0f\",\n",
        "    cmap=\"YlOrRd\",\n",
        "    ax=ax,\n",
        "    cbar_kws={\"label\": \"% of cluster\"},\n",
        ")\n",
        "ax.set_xlabel(\"Dominant Process\", fontsize=12)\n",
        "ax.set_ylabel(\"Hybrid Cluster\", fontsize=12)\n",
        "ax.set_title(\n",
        "    \"Dominant LSTM Process by Hybrid Cluster (h_n)\\nPercentage within each cluster\",\n",
        "    fontsize=14,\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig(\n",
        "    image_dir / \"lstm_v2_cluster_process_heatmap.png\", dpi=150, bbox_inches=\"tight\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save_md",
      "metadata": {},
      "source": [
        "## 11. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save_results",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Save all results\n",
        "# =============================================================================\n",
        "\n",
        "# Cell assignments\n",
        "h_cell_assignment.to_csv(table_dir / \"lstm_v2_hn_cell_assignment.csv\", index=False)\n",
        "c_cell_assignment.to_csv(table_dir / \"lstm_v2_cn_cell_assignment.csv\", index=False)\n",
        "\n",
        "# Comparison\n",
        "comparison_df.to_csv(table_dir / \"lstm_v2_hn_cn_comparison.csv\", index=False)\n",
        "\n",
        "# Gauge dominant process\n",
        "h_gauge_dominant.to_csv(table_dir / \"lstm_v2_gauge_dominant.csv\")\n",
        "\n",
        "# Hypothesis test\n",
        "hypothesis_df.to_csv(table_dir / \"lstm_v2_hypothesis_test.csv\", index=False)\n",
        "\n",
        "print(\"All results saved!\")\n",
        "print(f\"\\nOutput directory: {table_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary_md",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "1. **Per-gauge state analysis**: Each gauge's LSTM states were correlated with its own meteo data\n",
        "2. **Signed correlations**: Preserved positive/negative relationships\n",
        "3. **Z-score ranking**: Ensured balanced representation across processes\n",
        "4. **h_n vs c_n comparison**: Tested hypothesis about temporal scale specialization\n",
        "\n",
        "### Output Files\n",
        "\n",
        "**Tables**:\n",
        "- `lstm_v2_hn_cell_assignment.csv` - h_n cell assignments\n",
        "- `lstm_v2_cn_cell_assignment.csv` - c_n cell assignments\n",
        "- `lstm_v2_hn_cn_comparison.csv` - h_n vs c_n comparison statistics\n",
        "- `lstm_v2_gauge_dominant.csv` - Per-gauge dominant process\n",
        "- `lstm_v2_hypothesis_test.csv` - Temporal scale hypothesis results\n",
        "- `lstm_v2_cluster_process_crosstab.csv` - Cluster x process cross-tabulation\n",
        "\n",
        "**Figures**:\n",
        "- `lstm_v2_hn_cell_grid.png` - 16x16 grid of h_n cell assignments\n",
        "- `lstm_v2_hn_dominant_process_map.png` - Spatial map of dominant processes\n",
        "- `lstm_v2_hn_cn_comparison_grid.png` - Side-by-side h_n vs c_n grids\n",
        "- `lstm_v2_cluster_process_heatmap.png` - Cluster x process heatmap\n",
        "\n",
        "### Interpretation Notes\n",
        "\n",
        "- **h_n (hidden state)**: Should theoretically capture short-term dynamics\n",
        "- **c_n (cell state)**: Should theoretically capture long-term memory/seasonal patterns\n",
        "- Check `hypothesis_test.csv` to see if this pattern holds in the data"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "geo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
