{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df198fb",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.readers.geom_reader import load_geodata\n",
    "from src.readers.results_reader import read_conceptual_results, read_neural_results\n",
    "from src.timeseries_stats.metrics import (\n",
    "    calculate_bias,\n",
    "    calculate_kge,\n",
    "    calculate_mae,\n",
    "    calculate_nse,\n",
    "    calculate_rmse,\n",
    ")\n",
    "from src.utils.logger import setup_logger\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DeJavu Serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"]\n",
    "\n",
    "log = setup_logger(\"chapter_two\", log_file=\"../logs/chapter_two.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331195f",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f19b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load watershed geometries and gauge locations\n",
    "ws, gauges = load_geodata(folder_depth=\"../\")\n",
    "common_index = gauges.index.to_list()\n",
    "\n",
    "# Load cluster assignments (from Chapter 1)\n",
    "gauge_mapping = pd.read_csv(\n",
    "    \"../res/chapter_one/gauge_hybrid_mapping.csv\",\n",
    "    index_col=\"gauge_id\",\n",
    "    dtype={\"gauge_id\": str},\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(gauges)} gauges with hybrid classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780182b",
   "metadata": {},
   "source": [
    "## 2. Load Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GR4J results\n",
    "gr4j_dir = Path(\"../test/gr4j\")\n",
    "gr4j_results = read_conceptual_results(\n",
    "    gr4j_dir, model_name=\"GR4J\", common_index=common_index\n",
    ")\n",
    "\n",
    "# Load HBV results\n",
    "hbv_dir = Path(\"../test/hbv\")\n",
    "hbv_results = read_conceptual_results(\n",
    "    hbv_dir, model_name=\"HBV\", common_index=common_index\n",
    ")\n",
    "\n",
    "# Load RFR results\n",
    "rfr_dir = Path(\"../test/rfr\")\n",
    "rfr_results = read_conceptual_results(\n",
    "    rfr_dir, model_name=\"RFR\", common_index=common_index\n",
    ")\n",
    "\n",
    "# Load LSTM results\n",
    "lstm_dir = Path(\"../archive/neural_forecast\")\n",
    "lstm_results = read_neural_results(lstm_dir, common_index=common_index)\n",
    "\n",
    "print(f\"Loaded results for {len(gr4j_results)} gauges (GR4J)\")\n",
    "print(f\"Loaded results for {len(hbv_results)} gauges (HBV)\")\n",
    "print(f\"Loaded results for {len(rfr_results)} gauges (RFR)\")\n",
    "print(f\"Loaded results for {len(lstm_results)} gauges (LSTM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2891519",
   "metadata": {},
   "source": [
    "## 3. Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6c5472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_metrics(obs, sim):\n",
    "    \"\"\"Calculate NSE, KGE, RMSE, MAE, Bias.\"\"\"\n",
    "    return {\n",
    "        \"nse\": calculate_nse(obs, sim),\n",
    "        \"kge\": calculate_kge(obs, sim),\n",
    "        \"rmse\": calculate_rmse(obs, sim),\n",
    "        \"mae\": calculate_mae(obs, sim),\n",
    "        \"bias\": calculate_bias(obs, sim),\n",
    "    }\n",
    "\n",
    "\n",
    "# Compute metrics for all models\n",
    "metrics_data = []\n",
    "\n",
    "for gauge_id in common_index:\n",
    "    try:\n",
    "        # Extract observed data (same for all models)\n",
    "        obs = gr4j_results[gauge_id][\"obs\"]\n",
    "\n",
    "        # GR4J metrics\n",
    "        gr4j_sim = gr4j_results[gauge_id][\"sim\"]\n",
    "        gr4j_metrics = compute_all_metrics(obs, gr4j_sim)\n",
    "        metrics_data.append({\"gauge_id\": gauge_id, \"model\": \"GR4J\", **gr4j_metrics})\n",
    "\n",
    "        # HBV metrics\n",
    "        hbv_sim = hbv_results[gauge_id][\"sim\"]\n",
    "        hbv_metrics = compute_all_metrics(obs, hbv_sim)\n",
    "        metrics_data.append({\"gauge_id\": gauge_id, \"model\": \"HBV\", **hbv_metrics})\n",
    "\n",
    "        # RFR metrics\n",
    "        rfr_sim = rfr_results[gauge_id][\"sim\"]\n",
    "        rfr_metrics = compute_all_metrics(obs, rfr_sim)\n",
    "        metrics_data.append({\"gauge_id\": gauge_id, \"model\": \"RFR\", **rfr_metrics})\n",
    "\n",
    "        # LSTM metrics\n",
    "        if gauge_id in lstm_results:\n",
    "            lstm_sim = lstm_results[gauge_id][\"sim\"]\n",
    "            lstm_metrics = compute_all_metrics(obs, lstm_sim)\n",
    "            metrics_data.append({\"gauge_id\": gauge_id, \"model\": \"LSTM\", **lstm_metrics})\n",
    "    except Exception as e:\n",
    "        log.warning(f\"Error processing {gauge_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(f\"\\nCalculated metrics for {len(metrics_df)} gauge-model pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793e49c",
   "metadata": {},
   "source": [
    "## 4. Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics by model\n",
    "summary = metrics_df.groupby(\"model\").agg(\n",
    "    {\n",
    "        \"nse\": [\"mean\", \"median\", \"std\"],\n",
    "        \"kge\": [\"mean\", \"median\", \"std\"],\n",
    "        \"rmse\": [\"mean\", \"median\", \"std\"],\n",
    "        \"mae\": [\"mean\", \"median\", \"std\"],\n",
    "        \"bias\": [\"mean\", \"median\", \"std\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nOverall Model Performance (Mean ± Std):\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe33bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# NSE\n",
    "sns.boxplot(data=metrics_df, x=\"model\", y=\"nse\", ax=axes[0])\n",
    "axes[0].set_title(\"Nash-Sutcliffe Efficiency\")\n",
    "axes[0].axhline(0.5, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Threshold\")\n",
    "axes[0].legend()\n",
    "\n",
    "# KGE\n",
    "sns.boxplot(data=metrics_df, x=\"model\", y=\"kge\", ax=axes[1])\n",
    "axes[1].set_title(\"Kling-Gupta Efficiency\")\n",
    "axes[1].axhline(0.5, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Threshold\")\n",
    "axes[1].legend()\n",
    "\n",
    "# RMSE\n",
    "sns.boxplot(data=metrics_df, x=\"model\", y=\"rmse\", ax=axes[2])\n",
    "axes[2].set_title(\"Root Mean Squared Error\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../res/chapter_two/model_comparison_boxplots.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789f1e3",
   "metadata": {},
   "source": [
    "## 5. Performance by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e8b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with cluster assignments\n",
    "metrics_with_clusters = metrics_df.merge(\n",
    "    gauge_mapping[[\"hybrid_class\"]], left_on=\"gauge_id\", right_index=True\n",
    ")\n",
    "\n",
    "# Calculate mean NSE by cluster and model\n",
    "cluster_performance = (\n",
    "    metrics_with_clusters.groupby([\"hybrid_class\", \"model\"])[\"nse\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot(index=\"hybrid_class\", columns=\"model\", values=\"nse\")\n",
    ")\n",
    "\n",
    "print(\"\\nMean NSE by Cluster:\")\n",
    "print(cluster_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e88af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    cluster_performance,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"RdYlGn\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={\"label\": \"Mean NSE\"},\n",
    ")\n",
    "plt.title(\"Model Performance by Hybrid Class (NSE)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Hybrid Class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../res/chapter_two/cluster_performance_heatmap.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede230eb",
   "metadata": {},
   "source": [
    "## 6. Best Model by Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify best model for each cluster\n",
    "best_models = cluster_performance.idxmax(axis=1)\n",
    "print(\"\\nBest Model by Cluster (NSE):\")\n",
    "print(best_models)\n",
    "\n",
    "# Count model preferences\n",
    "model_counts = best_models.value_counts()\n",
    "print(\"\\nModel Preference Summary:\")\n",
    "print(model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af37206a",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f841e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export full metrics table\n",
    "metrics_df.to_csv(\"../res/chapter_two/model_metrics_all.csv\", index=False)\n",
    "\n",
    "# Export summary statistics\n",
    "summary.to_csv(\"../res/chapter_two/model_metrics_summary.csv\")\n",
    "\n",
    "# Export cluster performance\n",
    "cluster_performance.to_csv(\"../res/chapter_two/cluster_performance_nse.csv\")\n",
    "\n",
    "print(\"\\n✓ Exported performance metrics to res/chapter_two/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260a516",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Models evaluated:** GR4J, HBV, RFR, LSTM\n",
    "- **Metrics:** NSE, KGE, RMSE, MAE, Bias\n",
    "- **Cluster analysis:** Performance varies significantly across hybrid classes\n",
    "- **Key finding:** No single model dominates all regions — cluster-specific model selection recommended"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
